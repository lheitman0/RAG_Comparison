{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssFt2MqLck-Z",
        "outputId": "4f33acd9-bdea-4a0d-c2c4-ed460a0a8ef4"
      },
      "outputs": [],
      "source": [
        "!pip install -q PyMuPDF unstructured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7nDFDuU7d7P",
        "outputId": "b2002d41-f19d-4cbc-fdb8-0d19841e4a51"
      },
      "outputs": [],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fitz\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "import re\n",
        "\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rxsdmwx_bmZk"
      },
      "outputs": [],
      "source": [
        "pdf_1_file = open(pdf_1_path, 'rb')\n",
        "pdf_2_file = open(pdf_2_path, 'rb')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NpTrJZiLQE7H"
      },
      "outputs": [],
      "source": [
        "def dynamic_crop_above_caption(doc, pdf_name, page_num, caption_text, caption_rect, output_folder, fig_num, dpi=300):\n",
        "    page = doc[page_num]\n",
        "    x0, y0, x1, y1 = caption_rect\n",
        "    caption_y = y0\n",
        "\n",
        "    print(f\"Figura {fig_num}: Using provided caption rect on page {page_num + 1}: {caption_rect}\")\n",
        "\n",
        "    pix = page.get_pixmap(dpi=dpi)\n",
        "    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "    scale = dpi / 72\n",
        "\n",
        "    drawing_rects = []\n",
        "    for i, draw in enumerate(page.get_drawings()):\n",
        "        r = draw[\"rect\"]\n",
        "        lw = draw.get(\"linewidth\", 1.0)\n",
        "        stroke = draw.get(\"color\", (0, 0, 0))\n",
        "        fill = draw.get(\"fill\")\n",
        "\n",
        "        print(f\"   âž¤ Rect {i}: {r}, lw={lw}, fill={fill}, stroke={stroke}\")\n",
        "\n",
        "        if lw <= 1.0 and fill is None and stroke in [(0, 0, 0), (0.3, 0.3, 0.3)]:\n",
        "            print(f\"Passed visual filter\")\n",
        "            if abs(r.y1 - caption_y) < 100:\n",
        "                print(f\"Passed caption proximity: y1={r.y1:.2f}, caption_y={caption_y:.2f}\")\n",
        "                drawing_rects.append(r)\n",
        "            else:\n",
        "                print(f\"Failed caption proximity: y1={r.y1:.2f}, caption_y={caption_y:.2f}\")\n",
        "        else:\n",
        "            print(f\"Failed visual filter\")\n",
        "\n",
        "    if drawing_rects:\n",
        "        closest_rect = min(drawing_rects, key=lambda r: abs(r.y1 - caption_y))\n",
        "        crop_px = (\n",
        "            int(closest_rect.x0 * scale),\n",
        "            int(closest_rect.y0 * scale),\n",
        "            int(closest_rect.x1 * scale),\n",
        "            int(closest_rect.y1 * scale)\n",
        "        )\n",
        "        cropped = img.crop(crop_px)\n",
        "\n",
        "        filename = f\"{pdf_name}_figure_{fig_num}.png\"\n",
        "        out_path = os.path.join(output_folder, filename)\n",
        "        cropped.save(out_path)\n",
        "        print(f\"Cropped from vector box: figure {fig_num} on page {page_num + 1}\")\n",
        "\n",
        "        return {\n",
        "            \"filename\": filename,\n",
        "            \"figure_id\": f\"Figura {fig_num}\",\n",
        "            \"caption\": caption_text,\n",
        "            \"page\": page_num + 1,\n",
        "            \"rendered\": True,\n",
        "            \"source\": \"vector_box_crop\"\n",
        "        }\n",
        "\n",
        "    else:\n",
        "        print(\"No vector border found â€” trying embedded image fallback...\")\n",
        "        images = page.get_images(full=True)\n",
        "        valid_images = images[1:] if len(images) > 1 else []\n",
        "\n",
        "        if valid_images:\n",
        "            xref = valid_images[0][0]  # assume first non-header image\n",
        "            image_data = doc.extract_image(xref)\n",
        "            image_bytes = image_data[\"image\"]\n",
        "\n",
        "            filename = f\"{pdf_name}_figure_{fig_num}.png\"\n",
        "            out_path = os.path.join(output_folder, filename)\n",
        "            Image.open(io.BytesIO(image_bytes)).save(out_path)\n",
        "            print(f\"Saved embedded fallback image for figure {fig_num} on page {page_num + 1}\")\n",
        "\n",
        "            return {\n",
        "                \"filename\": filename,\n",
        "                \"figure_id\": f\"Figura {fig_num}\",\n",
        "                \"caption\": caption_text,\n",
        "                \"page\": page_num + 1,\n",
        "                \"rendered\": False,\n",
        "                \"source\": \"embedded_image_fallback\"\n",
        "            }\n",
        "\n",
        "        else:\n",
        "            print(\"No embedded image found either, skipping figure.\")\n",
        "            return None\n",
        "\n",
        "def extract_all_figures_with_captions(pdf_path, pdf_name, output_folder) -> dict:\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    doc = fitz.open(pdf_path)\n",
        "\n",
        "    figure_pattern = re.compile(r\"Figura\\s+(\\d+)\\s*[-â€“â€”]\\s*(.*)\", re.IGNORECASE)\n",
        "    figure_map = {}\n",
        "\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc[page_num]\n",
        "        blocks = page.get_text(\"blocks\")\n",
        "\n",
        "        for block in blocks:\n",
        "            text = block[4].strip()\n",
        "            match = figure_pattern.match(text)\n",
        "\n",
        "            if match:\n",
        "                fig_num = match.group(1)\n",
        "                caption = match.group(2).strip()\n",
        "                print(f\"ðŸ”Ž Found caption match: Figura {fig_num} â€” '{caption}' on page {page_num + 1}\")\n",
        "\n",
        "                if fig_num in figure_map:\n",
        "                    print(f\"Skipping Figura {fig_num} on page {page_num + 1} â€” already in map\")\n",
        "                    continue\n",
        "\n",
        "                caption_rect = block[:4]\n",
        "                crop_result = dynamic_crop_above_caption(doc, pdf_name, page_num, caption, caption_rect, output_folder, fig_num)\n",
        "                if crop_result:\n",
        "                    figure_map[fig_num] = crop_result\n",
        "\n",
        "    print(f\"Extracted {len(figure_map)} figures using vector-based cropping\")\n",
        "\n",
        "    metadata_path = os.path.join(output_folder, \"figure_metadata.json\")\n",
        "    with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(figure_map, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"Saved metadata to {metadata_path}\")\n",
        "    return figure_map\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "3sBx1mYSA1u3"
      },
      "outputs": [],
      "source": [
        "def sanitize_caption(caption):\n",
        "    return re.sub(r'[^\\w\\s-]', '', caption).strip().replace(' ', '_')[:60]\n",
        "\n",
        "def dynamic_crop_above_caption(doc, pdf_name, page_num, caption_text, caption_rect, output_folder, fig_num, dpi=300):\n",
        "    def process_page(page, caption_y, image_page_num, proximity_strategy=\"abs\", fallback=False):\n",
        "        pix = page.get_pixmap(dpi=dpi)\n",
        "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "        scale = dpi / 72\n",
        "        page_height = page.rect.height\n",
        "\n",
        "        drawing_rects = []\n",
        "        print(\" Scanning vector borders on page...\")\n",
        "        for i, draw in enumerate(page.get_drawings()):\n",
        "            r = draw[\"rect\"]\n",
        "            lw = draw.get(\"linewidth\", 1.0)\n",
        "            stroke = draw.get(\"color\", (0, 0, 0))\n",
        "            fill = draw.get(\"fill\")\n",
        "\n",
        "            print(f\"   âž¤ Rect {i}: {r}, lw={lw}, fill={fill}, stroke={stroke}\")\n",
        "\n",
        "            if lw <= 1.0 and fill is None and stroke in [(0, 0, 0), (0.3, 0.3, 0.3)]:\n",
        "                print(f\"Passed visual filter\")\n",
        "                if proximity_strategy == \"abs\":\n",
        "                    if abs(r.y1 - caption_y) < 100:\n",
        "                        print(f\"Passed caption proximity: y1={r.y1:.2f}, caption_y={caption_y:.2f}\")\n",
        "                        drawing_rects.append(r)\n",
        "                    else:\n",
        "                        print(f\"Failed caption proximity\")\n",
        "                elif proximity_strategy == \"bleed\":\n",
        "                    bottom_margin = 150  # pixels from bottom considered for bleed-over\n",
        "                    if (page_height - r.y1) < bottom_margin:\n",
        "                        print(f\"Passed bleed-over proximity (y1={r.y1:.2f}, page_height={page_height:.2f})\")\n",
        "                        drawing_rects.append(r)\n",
        "                    else:\n",
        "                        print(f\"Failed bleed-over proximity (y1={r.y1:.2f} >= {bottom_margin:.2f})\")\n",
        "            else:\n",
        "                print(f\"Failed visual filter\")\n",
        "\n",
        "        if drawing_rects:\n",
        "            closest_rect = min(drawing_rects, key=lambda r: abs(r.y1 - caption_y))\n",
        "            crop_px = (\n",
        "                int(closest_rect.x0 * scale),\n",
        "                int(closest_rect.y0 * scale),\n",
        "                int(closest_rect.x1 * scale),\n",
        "                int(closest_rect.y1 * scale)\n",
        "            )\n",
        "            cropped = img.crop(crop_px)\n",
        "\n",
        "            filename = f\"{pdf_name}_figure_{fig_num}.png\"  # fig_num here is the full unique_id e.g., \"69_VMware_NSX\"\n",
        "            out_path = os.path.join(output_folder, filename)\n",
        "            cropped.save(out_path)\n",
        "            print(f\"Cropped from vector box: figure {fig_num} from page {image_page_num + 1}\")\n",
        "\n",
        "            return {\n",
        "                \"filename\": filename,\n",
        "                \"figure_id\": f\"Figura {fig_num}\",\n",
        "                \"caption\": caption_text,\n",
        "                \"page\": page_num + 1,\n",
        "                \"image_page\": image_page_num + 1,\n",
        "                \"rendered\": True,\n",
        "                \"source\": \"bleed_over_crop\" if fallback else \"vector_box_crop\"\n",
        "            }\n",
        "        return None\n",
        "\n",
        "    page = doc[page_num]\n",
        "    x0, y0, x1, y1 = caption_rect\n",
        "    caption_y = y0\n",
        "\n",
        "    print(f\"\\n[Figura {fig_num}] Using provided caption rect on page {page_num + 1}: {caption_rect}\")\n",
        "    result = process_page(page, caption_y, page_num, proximity_strategy=\"abs\")\n",
        "\n",
        "    if result:\n",
        "        return result\n",
        "\n",
        "    caption_is_top_of_page = caption_y < 200\n",
        "\n",
        "    if caption_is_top_of_page and page_num > 0:\n",
        "        print(\"Caption is high on the page â€” trying previous page first (bleed-over)...\")\n",
        "        prev_page = doc[page_num - 1]\n",
        "        result = process_page(prev_page, caption_y, page_num - 1, proximity_strategy=\"bleed\", fallback=True)\n",
        "        if result:\n",
        "            return result\n",
        "\n",
        "    # Embedded image fallback â€” select the one closest *above* the caption \n",
        "    print(\"Trying embedded image fallback on caption page (using top-aligned matching)...\")\n",
        "    image_infos = page.get_image_info(xrefs=True)\n",
        "\n",
        "    # Filter only images whose top is above the caption\n",
        "    image_above_caption = [\n",
        "        info for info in image_infos\n",
        "        if info[\"bbox\"][1] < caption_y\n",
        "    ]\n",
        "\n",
        "    print(f\"Found {len(image_infos)} embedded images with position info on page {page_num + 1}\")\n",
        "    print(f\"Caption Y: {caption_y}\")\n",
        "    for info in image_above_caption:\n",
        "        print(f\"   âž¤ Image bbox: {info['bbox']}\")\n",
        "\n",
        "    if image_above_caption:\n",
        "        closest_image = max(image_above_caption, key=lambda info: info[\"bbox\"][1])\n",
        "        xref = closest_image[\"xref\"]\n",
        "\n",
        "        image_data = doc.extract_image(xref)\n",
        "        image_bytes = image_data[\"image\"]\n",
        "\n",
        "        filename = f\"{pdf_name}_figure_{fig_num}.png\"\n",
        "        out_path = os.path.join(output_folder, filename)\n",
        "        Image.open(io.BytesIO(image_bytes)).save(out_path)\n",
        "        print(f\"Saved embedded image just above caption for figure {fig_num} on page {page_num + 1}\")\n",
        "\n",
        "        return {\n",
        "            \"filename\": filename,\n",
        "            \"figure_id\": f\"Figura {fig_num}\",\n",
        "            \"caption\": caption_text,\n",
        "            \"page\": page_num + 1,\n",
        "            \"image_page\": page_num + 1,\n",
        "            \"rendered\": False,\n",
        "            \"source\": \"embedded_image_fallback\"\n",
        "        }\n",
        "\n",
        "    print(\"No embedded image found either, skipping figure\")\n",
        "    return None\n",
        "\n",
        "def extract_all_figures_with_captions(pdf_path, pdf_name, output_folder) -> dict:\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    doc = fitz.open(pdf_path)\n",
        "\n",
        "    figure_pattern = re.compile(r\"Figura\\s+(\\d+)\\s*[-â€“â€”]\\s*(.*)\", re.IGNORECASE)\n",
        "    figure_map = {}\n",
        "    seen_figures = set()\n",
        "\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc[page_num]\n",
        "        blocks = page.get_text(\"blocks\")\n",
        "\n",
        "        for block in blocks:\n",
        "            text = block[4].strip()\n",
        "            match = figure_pattern.match(text)\n",
        "\n",
        "            if match:\n",
        "                fig_num = match.group(1)\n",
        "                caption = match.group(2).strip().lower()\n",
        "                figure_key = (fig_num, caption)\n",
        "\n",
        "                print(f\"ðŸ”Ž Found caption match: Figura {fig_num} â€” '{caption}' on page {page_num + 1}\")\n",
        "\n",
        "                safe_caption = sanitize_caption(caption)\n",
        "                unique_id = f\"{fig_num}_{safe_caption}\"\n",
        "\n",
        "                if unique_id in figure_map:\n",
        "                    print(f\"Skipping Figura {fig_num} â€” duplicate caption\")\n",
        "                    continue\n",
        "\n",
        "                caption_rect = block[:4]\n",
        "\n",
        "                crop_result = dynamic_crop_above_caption(\n",
        "                    doc, pdf_name, page_num, caption, caption_rect,\n",
        "                    output_folder, unique_id  # used for both image name + key\n",
        "                )\n",
        "\n",
        "                if crop_result:\n",
        "                    figure_map[unique_id] = crop_result\n",
        "\n",
        "\n",
        "    print(f\"Extracted {len(figure_map)} figures using vector-based cropping\")\n",
        "\n",
        "    metadata_path = os.path.join(output_folder, \"figure_metadata.json\")\n",
        "    with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(figure_map, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"Saved metadata to {metadata_path}\")\n",
        "    return figure_map\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0gy4YJpbI62",
        "outputId": "dafd1abb-ebbe-45a5-bbbc-d1a01d6f0773"
      },
      "outputs": [],
      "source": [
        "pdf_path = '/data/VM_manual/ISTRUZIONE_OPERATIVA_CREAZIONE_VM_CLOUD_INSIEL_REV_00.pdf'\n",
        "output_folder_path = '/data/VM_manual'\n",
        "pdf_name = 'VM_manual'\n",
        "\n",
        "figures_folder = os.path.join(output_folder_path, 'figures')\n",
        "os.makedirs(figures_folder, exist_ok=True)\n",
        "\n",
        "wifi_figure_map = extract_all_figures_with_captions(\n",
        "    pdf_path=pdf_path,\n",
        "    pdf_name=pdf_name,\n",
        "    output_folder=figures_folder \n",
        ")\n",
        "\n",
        "metadata_path = os.path.join(output_folder_path, \"figure_metadata.json\")\n",
        "with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(wifi_figure_map, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Saved figure metadata to: {metadata_path}\")\n",
        "print(\"Sample extracted figures:\")\n",
        "for fig in list(wifi_figure_map.values())[:5]:\n",
        "    print(fig)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICtIu0ofb7Yx",
        "outputId": "f5751a2b-9b11-41f6-a39f-89f12176271a"
      },
      "outputs": [],
      "source": [
        "pdf_path = '/data/wifi_manual/ISTRUZIONE_OPERATIVA_CONFIGURAZIONE_WIFI_ARUBA_REV_01.pdf'\n",
        "output_folder_path = '/data/wifi_manual'\n",
        "pdf_name = 'wifi_manual'\n",
        "\n",
        "figures_folder = os.path.join(output_folder_path, 'figures')\n",
        "os.makedirs(figures_folder, exist_ok=True)\n",
        "\n",
        "wifi_figure_map = extract_all_figures_with_captions(\n",
        "    pdf_path=pdf_path,\n",
        "    pdf_name=pdf_name,\n",
        "    output_folder=figures_folder  \n",
        ")\n",
        "\n",
        "metadata_path = os.path.join(output_folder_path, \"figure_metadata.json\")\n",
        "with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(wifi_figure_map, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Saved figure metadata to: {metadata_path}\")\n",
        "print(\"Sample extracted figures:\")\n",
        "for fig in list(wifi_figure_map.values())[:5]:\n",
        "    print(fig)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPAeaE2xks49"
      },
      "source": [
        "# Semantic chunking w/ llama parse + gpt 4 and instructor (?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGPWtJN-so3b",
        "outputId": "d09fcec7-8ebf-471d-d010-cf3606a262b4"
      },
      "outputs": [],
      "source": [
        "!pip install llama-parse --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "LFuCFW8ukwrG"
      },
      "outputs": [],
      "source": [
        "def parse_pdf_llama_no_image_text(filename: str, output_path: str):\n",
        "    print(f\"Parsing '{filename}' (OCR=OFF, imageâ€‘text filtered)...\")\n",
        "\n",
        "    parser = LlamaParse(\n",
        "        api_key=os.environ[\"LLAMA_CLOUD_API_KEY\"],\n",
        "        result_type=\"markdown\",\n",
        "        include_metadata=True,\n",
        "        include_images=False,\n",
        "        parse_mode=\"parse_page_with_agent\",\n",
        "        disable_image_extraction=True,\n",
        "        disable_ocr=True,\n",
        "        chunking_strategy=\"hierarchical\",\n",
        "        auto_title_generation=False,\n",
        "        infer_table_structures=False,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "    documents = parser.load_data(filename)\n",
        "\n",
        "    filtered = []\n",
        "    for doc in documents:\n",
        "        if doc.metadata.get(\"block_type\") != \"image\":\n",
        "            filtered.append(doc)\n",
        "\n",
        "    md_text = \"\\n\\n\".join([d.text for d in filtered])\n",
        "\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(md_text)\n",
        "\n",
        "    print(f\"Saved filtered markdown to: {output_path}\")\n",
        "    return md_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "KFp9Be8CsjvG",
        "outputId": "a0d7fc9d-91ba-453a-ae25-a78ad4132a51"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import os\n",
        "from llama_parse import LlamaParse\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "os.environ[\"LLAMA_CLOUD_API_KEY\"] = LLAMA_CLOUD_API_KEY\n",
        "\n",
        "filename    = '/data/wifi_manual/ISTRUZIONE_OPERATIVA_CONFIGURAZIONE_WIFI_ARUBA_REV_01.pdf'\n",
        "output_path = os.path.join(\n",
        "    os.path.dirname(filename),\n",
        "    os.path.basename(filename).replace(\".pdf\", \"_parsed_no_image_llm.md\")\n",
        ")\n",
        "\n",
        "md_result = parse_pdf_llama_no_image_text(filename, output_path)\n",
        "\n",
        "\n",
        "display(Markdown(md_result[:2000]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "id": "AcVeF4G8BhrC",
        "outputId": "36fe8576-92a5-40c5-912a-16abb2a0ffd6"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import os\n",
        "from llama_parse import LlamaParse\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "\n",
        "os.environ[\"LLAMA_CLOUD_API_KEY\"] = LLAMA_CLOUD_API_KEY\n",
        "# Usage\n",
        "filename    = '/data/VM_manual/ISTRUZIONE_OPERATIVA_CREAZIONE_VM_CLOUD_INSIEL_REV_00.pdf'\n",
        "output_path = os.path.join(\n",
        "    os.path.dirname(filename),\n",
        "    os.path.basename(filename).replace(\".pdf\", \"_parsed_no_image_llm.md\")\n",
        ")\n",
        "\n",
        "md_result = parse_pdf_llama_no_image_text(filename, output_path)\n",
        "\n",
        "# Preview first bit\n",
        "display(Markdown(md_result[:2000]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ymueZe2NaY5f"
      },
      "outputs": [],
      "source": [
        "def strip_artifacts(md_text: str) -> str:\n",
        "    # 1) Remove mermaid code fences\n",
        "    md_text = re.sub(r'```mermaid\\b.*?```', '', md_text, flags=re.DOTALL|re.IGNORECASE)\n",
        "    # 2) Drop any fenced code blocks (```...```)\n",
        "    md_text = re.sub(r'```[\\s\\S]*?```', '', md_text)\n",
        "    # 3) Remove ASCIIâ€‘tree lines\n",
        "    md_text = re.sub(r'(?m)^[ \\t]*(?:[â”œâ””â”‚].*|\\+--.*)$', '', md_text)\n",
        "    # 4) Remove pure markdown table rows\n",
        "    md_text = re.sub(r'(?m)^\\|.*\\|\\s*$', '', md_text)\n",
        "    return md_text\n",
        "\n",
        "# Split pdf into pages \n",
        "def split_into_pages(md_text: str) -> list[tuple[int, str]]:\n",
        "    pattern = re.compile(\n",
        "        r'(Pagina\\s+(\\d+)\\s+di\\s+\\d+.*?)(?=Pagina\\s+\\d+\\s+di\\s+\\d+|\\Z)',\n",
        "        flags=re.DOTALL | re.IGNORECASE\n",
        "    )\n",
        "    chunks = pattern.findall(md_text)\n",
        "\n",
        "    pages = []\n",
        "    for full_block, page_num in chunks:\n",
        "        pages.append((int(page_num), full_block))\n",
        "    return pages\n",
        "\n",
        "\n",
        "def extract_sections_from_index_pages(md_text: str, page_nums: list[int]) -> set[str]:\n",
        "    pages = split_into_pages(md_text)\n",
        "    blocks = [b for num, b in pages if num in page_nums]\n",
        "\n",
        "    if not blocks:\n",
        "        raise ValueError(f\"No index pages found for {page_nums}\")\n",
        "\n",
        "    nums = set()\n",
        "    for block in blocks:\n",
        "        for line in block.splitlines():\n",
        "            m = re.match(r'^\\s*(\\d+(?:\\.\\d+)*)\\.', line)\n",
        "            if m:\n",
        "                nums.add(m.group(1))\n",
        "    return nums\n",
        "\n",
        "\n",
        "def promote_numeric_headings(parsed_md: str, valid_sections: set[str]) -> str:\n",
        "    pattern = re.compile(\n",
        "        r'^(?P<hashes>#{0,6}\\s*)?(?P<num>\\d+(?:\\.\\d+)*)(?:\\.)?\\s+(?P<title>.*)$',\n",
        "        re.MULTILINE\n",
        "    )\n",
        "\n",
        "    def _repl(m):\n",
        "        hashes = m.group(\"hashes\") or ''\n",
        "        num = m.group(\"num\")\n",
        "        title = m.group(\"title\").strip()\n",
        "\n",
        "        if num not in valid_sections:\n",
        "            return m.group(0)\n",
        "\n",
        "        if title.endswith(':'):\n",
        "            return m.group(0)\n",
        "\n",
        "        letters = re.findall(r'[A-Za-z]', title)\n",
        "        if letters:\n",
        "            lower_frac = sum(1 for c in letters if c.islower()) / len(letters)\n",
        "            if lower_frac > 0.4:\n",
        "                return m.group(0)\n",
        "\n",
        "        level = num.count('.') + 1\n",
        "        return f\"{'#' * level} {num} {title}\"\n",
        "\n",
        "    return pattern.sub(_repl, parsed_md)\n",
        "\n",
        "\n",
        "\n",
        "def process_page(page_md: str, page_num: int, valid_sections: set[str], header_skip_patterns: list[re.Pattern]) -> str:\n",
        "    lines = page_md.splitlines()\n",
        "    out = []\n",
        "    page_line = None\n",
        "    date_str = None\n",
        "\n",
        "    for L in lines:\n",
        "        if any(pat.match(L) for pat in header_skip_patterns):\n",
        "            m_dt = re.search(r'ISTRUZIONE OPERATIVA.*?([0-9]{2}/[0-9]{2}/[0-9]{4})', L, flags=re.IGNORECASE)\n",
        "            if m_dt:\n",
        "                date_str = m_dt.group(1)\n",
        "            continue\n",
        "        m_pg = re.match(r'^\\s*Pagina\\s+(\\d+)\\s+di\\s+(\\d+)', L, flags=re.IGNORECASE)\n",
        "        if m_pg:\n",
        "            page_num_str = m_pg.group(1)\n",
        "            total_pages = m_pg.group(2)\n",
        "            page_line = f\"*Pagina {page_num_str} di {total_pages}\"\n",
        "            continue\n",
        "\n",
        "        out.append(L)\n",
        "\n",
        "    if page_line:\n",
        "        meta = page_line\n",
        "        if date_str:\n",
        "            meta += f\" ({date_str})*\"\n",
        "        out.insert(0, '')        \n",
        "        out.insert(1, meta)      \n",
        "        out.insert(2, '')        \n",
        "\n",
        "\n",
        "\n",
        "    body = \"\\n\".join(out)\n",
        "\n",
        "    if page_num > 2:\n",
        "        body = strip_artifacts(body)\n",
        "\n",
        "    body = promote_numeric_headings(body, valid_sections)\n",
        "    return body\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OLJ9YSvDxLuw"
      },
      "outputs": [],
      "source": [
        "wifi_skip_patterns = [\n",
        "    re.compile(r'^\\s*Configurazione Wi-Fi Aruba:', re.IGNORECASE),\n",
        "    re.compile(r'^\\s*access point e terminali', re.IGNORECASE),\n",
        "    re.compile(r'^\\s*ISTRUZIONE OPERATIVA\\s*\\d{2}/\\d{2}/\\d{4}', re.IGNORECASE),\n",
        "]\n",
        "\n",
        "vm_skip_patterns = [\n",
        "    re.compile(r'^\\s*Creazione VM su Cloud INSIEL.*$', re.IGNORECASE),\n",
        "    re.compile(r'^\\s*ISTRUZIONE OPERATIVA\\s*\\d{2}/\\d{2}/\\d{4}', re.IGNORECASE),\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTuyR90r5VH4",
        "outputId": "c7dd02d5-09d0-43ab-aea5-afea8fc07937"
      },
      "outputs": [],
      "source": [
        "md_path = \"/data/wifi_manual/ISTRUZIONE_OPERATIVA_CONFIGURAZIONE_WIFI_ARUBA_REV_01_parsed_no_image_llm.md\"\n",
        "output_path = \"/data/wifi_manual/ISTRUZIONE_OPERATIVA_CONFIGURAZIONE_WIFI_ARUBA_REV_01_cleaned.md\"\n",
        "\n",
        "\n",
        "with open(md_path, 'r', encoding='utf-8') as f:\n",
        "    raw_md = f.read()\n",
        "\n",
        "\n",
        "\n",
        "toc_sections = extract_sections_from_index_pages(raw_md, page_nums=[3])\n",
        "print(\"Valid sections:\", sorted(toc_sections)[:10], \"â€¦\")\n",
        "\n",
        "\n",
        "pages = split_into_pages(raw_md)\n",
        "processed = [\n",
        "    process_page(chunk, num, toc_sections, header_skip_patterns=wifi_skip_patterns)\n",
        "    for num, chunk in pages\n",
        "]\n",
        "\n",
        "final_md = \"\\n\\n\".join(processed)\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(final_md)\n",
        "\n",
        "print(\"Cleaned Markdown written to\", output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnlCtGC7OdBk",
        "outputId": "3e5aaec1-8b7c-4bad-b043-b6903f6606e5"
      },
      "outputs": [],
      "source": [
        "md_path = \"/data/VM_manual/ISTRUZIONE_OPERATIVA_CREAZIONE_VM_CLOUD_INSIEL_REV_00_parsed_no_image_llm.md\"\n",
        "output_path = \"/data/VM_manual/ISTRUZIONE_OPERATIVA_CREAZIONE_VM_CLOUD_INSIEL_REV_00_cleaned.md\"\n",
        "\n",
        "with open(md_path, 'r', encoding='utf-8') as f:\n",
        "    raw_md = f.read()\n",
        "\n",
        "\n",
        "# Get true section numbers from the INDICE on pageÂ 3!!\n",
        "toc_sections = extract_sections_from_index_pages(raw_md, page_nums=[3, 4])\n",
        "print(\"Valid sections:\", sorted(toc_sections)[:10], \"â€¦\")\n",
        "\n",
        "pages = split_into_pages(raw_md)\n",
        "processed = [\n",
        "    process_page(chunk, num, toc_sections, header_skip_patterns=vm_skip_patterns)\n",
        "    for num, chunk in pages\n",
        "]\n",
        "\n",
        "final_md = \"\\n\\n\".join(processed)\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(final_md)\n",
        "\n",
        "print(\"Cleaned Markdown written to\", output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybz2i6aYkuek",
        "outputId": "641ab559-5dd2-4356-cf7f-db88577ddb10"
      },
      "outputs": [],
      "source": [
        "!pip install openai instructor --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uAycFRmKuyBc"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import instructor\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional, Dict\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class FigureMetadata(BaseModel):\n",
        "    figure_id: str = Field(..., description=\"Unique identifier for the figure\")\n",
        "    filename: str = Field(..., description=\"Name of the image file\")\n",
        "    caption: str = Field(..., description=\"Caption text for the figure\")\n",
        "    page: int = Field(..., description=\"Page number where the figur caption is located\")\n",
        "    image_page: int = Field(..., description=\"Page number where the image is located\")\n",
        "    rendered: bool = Field(..., description=\"Whether the image has been rendered\")\n",
        "    source: str = Field(..., description=\"Source of the image (e.g., 'vector_box_crop')\")\n",
        "\n",
        "\n",
        "class SemanticChunk(BaseModel):\n",
        "    section_number: str = Field(..., description=\"Hierarchical numeric section (e.g., 5.1.3.1.3)\")\n",
        "    section_title: str = Field(..., description=\"Concise and meaningful title for the section\")\n",
        "    page_numbers: List[int] = Field(..., description=\"Page numbers included in the chunk\")\n",
        "    content: str = Field(..., description=\"Full markdown content of the chunk\")\n",
        "    parent_topics: List[str] = Field(..., description=\"Ordered list of parent section titles\")\n",
        "    document: str = Field(..., description=\"Name or identifier of the document\")\n",
        "    chunk_summary: str = Field(..., description=\"A summary of the chunk content\")\n",
        "    figures: Optional[List[FigureMetadata]] = Field(default_factory=list, description=\"Figures referenced explicitly in this chunk\")\n",
        "\n",
        "class ChunkSet(BaseModel):\n",
        "    chunks: List[SemanticChunk] = Field(..., description=\"List of semantic chunks\")\n",
        "\n",
        "class SummaryOnlyChunk(BaseModel):\n",
        "    chunk_summary: str = Field(..., description=\"A summary of the chunk content\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "t1HkcV0FZ_Rx"
      },
      "outputs": [],
      "source": [
        "def summarize_chunks(chunks: List[SemanticChunk]) -> List[SemanticChunk]:\n",
        "    enriched = []\n",
        "\n",
        "    for chunk in tqdm(chunks, desc=\"Summarizing chunks\"):\n",
        "        figure_count = len(chunk.figures)\n",
        "        figure_desc = \"\"\n",
        "        if figure_count:\n",
        "            figure_titles = \", \".join([f.caption for f in chunk.figures])\n",
        "            figure_desc = f\"\\nThis section includes {figure_count} figure(s): {figure_titles}.\"\n",
        "\n",
        "        parent_path = \" > \".join(chunk.parent_topics + [chunk.section_title])\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "You are helping document a technical manual. Below is a markdown section titled:\n",
        "\n",
        "{parent_path}\n",
        "\n",
        "Please write a clear, concise **summary** of this section for retrieval-augmented systems.\n",
        "\n",
        "Guidelines:\n",
        "- Summarize the purpose of the section\n",
        "- Note any configuration steps, troubleshooting, or systems mentioned\n",
        "- If figures are included, mention how they support the content\n",
        "{figure_desc}\n",
        "\n",
        "Markdown content:\n",
        "{chunk.content}\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            summary_obj = client.chat.completions.create(\n",
        "                model=\"gpt-4-turbo\",\n",
        "                response_model=SummaryOnlyChunk,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.3,\n",
        "            )\n",
        "\n",
        "            chunk.chunk_summary = summary_obj.chunk_summary\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to summarize section {chunk.section_number}: {e}\")\n",
        "            chunk.chunk_summary = \"\"\n",
        "\n",
        "        enriched.append(chunk)\n",
        "\n",
        "    return enriched\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "L-5bMED7JGPh"
      },
      "outputs": [],
      "source": [
        "def parse_heading_structure(md_text: str) -> List[Dict]:\n",
        "    headings = []\n",
        "    lines = md_text.splitlines()\n",
        "\n",
        "    for idx, line in enumerate(lines):\n",
        "        match = re.match(r'^(#{1,6})\\s+(.*)', line)\n",
        "        if match:\n",
        "            level = len(match.group(1))\n",
        "            raw_text = match.group(2).strip()\n",
        "\n",
        "            if re.search(r'\\.{5,}\\s*\\d+\\s*$', raw_text):\n",
        "                continue\n",
        "\n",
        "            sec_match = re.match(r'^([\\d\\.]+)\\.?\\s*(.*)', raw_text)\n",
        "            if not sec_match:\n",
        "                continue\n",
        "\n",
        "            section_number = sec_match.group(1)\n",
        "            title = sec_match.group(2).strip()\n",
        "\n",
        "            headings.append({\n",
        "                \"line_number\": idx,\n",
        "                \"level\": level,\n",
        "                \"raw\": line.strip(),\n",
        "                \"text\": raw_text,\n",
        "                \"section_number\": section_number,\n",
        "                \"title\": title\n",
        "            })\n",
        "\n",
        "    return headings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lRi8i83nKFEn"
      },
      "outputs": [],
      "source": [
        "def resolve_parent_topics(headings: List[Dict]) -> List[Dict]:\n",
        "    stack = []\n",
        "    result = []\n",
        "\n",
        "    for heading in headings:\n",
        "        level = heading[\"level\"]\n",
        "\n",
        "        stack = [h for h in stack if h[\"level\"] < level]\n",
        "\n",
        "        stack.append(heading)\n",
        "\n",
        "        parent_topics = [h[\"title\"] for h in stack[:-1]]\n",
        "\n",
        "        heading_with_parents = heading.copy()\n",
        "        heading_with_parents[\"parent_topics\"] = parent_topics\n",
        "        result.append(heading_with_parents)\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hwMRbrnSKoUX"
      },
      "outputs": [],
      "source": [
        "from difflib import get_close_matches\n",
        "\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "def generate_semantic_chunks_from_headings(\n",
        "    md_text: str,\n",
        "    headings: List[Dict],\n",
        "    document_name: str,\n",
        "    line_to_page: Dict[int, int],\n",
        "    original_lines: Optional[List[str]] = None\n",
        ") -> List[Dict]:\n",
        "\n",
        "    lines = md_text.splitlines()\n",
        "    chunks = []\n",
        "\n",
        "    def get_chunk_page_range(start: int, end: int, mapping: Dict[int, int]) -> List[int]:\n",
        "        pages = [mapping.get(i) for i in range(start, end)]\n",
        "        return sorted(set(p for p in pages if p is not None))\n",
        "\n",
        "    for i, heading in enumerate(headings):\n",
        "        start_line = heading[\"line_number\"]\n",
        "        end_line = headings[i + 1][\"line_number\"] if i + 1 < len(headings) else len(lines)\n",
        "\n",
        "        chunk_lines = lines[start_line:end_line]\n",
        "        chunk_text = \"\\n\".join(chunk_lines).strip()\n",
        "\n",
        "        page_numbers = get_chunk_page_range(start_line, end_line, line_to_page)\n",
        "\n",
        "        chunks.append({\n",
        "            \"section_number\": heading[\"section_number\"],\n",
        "            \"section_title\": heading[\"title\"],\n",
        "            \"page_numbers\": page_numbers,\n",
        "            \"content\": chunk_text,\n",
        "            \"parent_topics\": heading[\"parent_topics\"],\n",
        "            \"document\": document_name,\n",
        "            \"chunk_summary\": \"\",     \n",
        "            \"figures\": []           \n",
        "        })\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "\n",
        "def match_caption_to_content(fig, content: str) -> bool:\n",
        "    lines = re.findall(r\"(Figura\\s+\\d+\\s*[-â€“â€”]\\s*[^\\n]+)\", content, flags=re.IGNORECASE)\n",
        "    fig_caption = fig[\"caption\"].lower()\n",
        "    return any(fig_caption in line.lower() for line in lines)\n",
        "\n",
        "\n",
        "\n",
        "def attach_figures_to_chunks(chunks: List[SemanticChunk], figure_metadata_dict: dict) -> List[SemanticChunk]:\n",
        "    figure_metadata = list(figure_metadata_dict.values())\n",
        "    fig_index = {}\n",
        "    for fig in figure_metadata:\n",
        "        match = re.search(r\"Figura\\s+(\\d+)\", fig[\"figure_id\"])\n",
        "        if match:\n",
        "            fig_index.setdefault(match.group(1), []).append(fig)\n",
        "\n",
        "    for chunk in chunks:\n",
        "        matched_figures = []\n",
        "        seen_ids = set()\n",
        "        figure_nums = re.findall(r\"Figura\\s+(\\d+)\", chunk.content, flags=re.IGNORECASE)\n",
        "\n",
        "        for num in figure_nums:\n",
        "            for fig in fig_index.get(num, []):\n",
        "                if fig[\"figure_id\"] not in seen_ids and match_caption_to_content(fig, chunk.content):\n",
        "                    matched_figures.append(FigureMetadata(**fig))\n",
        "                    seen_ids.add(fig[\"figure_id\"])\n",
        "\n",
        "        chunk.figures = matched_figures\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "j3IRnGdZHqnX"
      },
      "outputs": [],
      "source": [
        "def build_line_to_page_map(lines: List[str]) -> dict:\n",
        "    line_to_page = {}\n",
        "    current_page = None\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        match = re.search(r\"Pagina\\s+(\\d+)\\s+di\\s+\\d+\", line, flags=re.IGNORECASE)\n",
        "        if match:\n",
        "            current_page = int(match.group(1))\n",
        "        if current_page:\n",
        "            line_to_page[i] = current_page\n",
        "\n",
        "    return line_to_page\n",
        "\n",
        "def strip_page_markers(lines: List[str]) -> List[str]:\n",
        "    return [line for line in lines if not re.search(r\"Pagina\\s+\\d+\\s+di\\s+\\d+\", line, re.IGNORECASE)]\n",
        "\n",
        "def get_chunk_page_range(start_line: int, end_line: int, line_to_page: dict) -> List[int]:\n",
        "    pages = [line_to_page.get(i) for i in range(start_line, end_line)]\n",
        "    return sorted(set(p for p in pages if p is not None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "W2lfq0w-9LRI"
      },
      "outputs": [],
      "source": [
        "def normalize_section_number(sec: str) -> str:\n",
        "    return sec.strip().rstrip(\".\")\n",
        "\n",
        "def deduplicate_by_section_number(chunks: List[SemanticChunk]) -> List[SemanticChunk]:\n",
        "    seen = {}\n",
        "    removed = []\n",
        "    for chunk in chunks:\n",
        "        key = normalize_section_number(chunk.section_number)\n",
        "        if key in seen:\n",
        "            removed.append(seen[key])\n",
        "        seen[key] = chunk\n",
        "    return list(seen.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_SoqBwwXuIX"
      },
      "source": [
        "## GENERATE WIFI CHUNKS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPPHoACVOnr_",
        "outputId": "60de0e96-4174-4e69-ea93-6cabe9ce624c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Summarizing chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [06:13<00:00,  8.11s/it]\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "from instructor import from_openai\n",
        "import json\n",
        "\n",
        "client = from_openai(OpenAI(api_key=OPENAI_API_KEY))\n",
        "\n",
        "\n",
        "md_path = \"/data/wifi_manual/ISTRUZIONE_OPERATIVA_CONFIGURAZIONE_WIFI_ARUBA_REV_01_cleaned.md\"\n",
        "document_name = \"ISTRUZIONE OPERATIVA CONFIGURAZIONE WIFI ARUBA REV 01\"\n",
        "figure_metadata_path = '/content/drive/MyDrive/rag_comparison/data/wifi_manual/figure_metadata.json'\n",
        "final_json_path = '/data/wifi_manual/final_wifi_chunks.json'\n",
        "\n",
        "with open(md_path, 'r', encoding='utf-8') as f:\n",
        "    markdown_text = f.read()\n",
        "\n",
        "lines = markdown_text.splitlines()\n",
        "line_to_page = build_line_to_page_map(lines)\n",
        "stripped_lines = strip_page_markers(lines)\n",
        "stripped_markdown = \"\\n\".join(stripped_lines)\n",
        "\n",
        "parsed_headings = parse_heading_structure(markdown_text)\n",
        "\n",
        "parsed_headings = parse_heading_structure(markdown_text)\n",
        "headings_with_parents = resolve_parent_topics(parsed_headings)\n",
        "\n",
        "\n",
        "raw_chunks = generate_semantic_chunks_from_headings(\n",
        "    stripped_markdown,\n",
        "    headings_with_parents,\n",
        "    document_name,\n",
        "    line_to_page=line_to_page,\n",
        "    original_lines=lines  \n",
        ")\n",
        "\n",
        "\n",
        "semantic_chunks = [SemanticChunk(**chunk) for chunk in raw_chunks]\n",
        "\n",
        "with open(figure_metadata_path, 'r') as f:\n",
        "    figure_metadata_dict = json.load(f)\n",
        "\n",
        "semantic_chunks_with_figures = attach_figures_to_chunks(semantic_chunks, figure_metadata_dict)\n",
        "\n",
        "summarized_chunks = summarize_chunks(semantic_chunks_with_figures)\n",
        "\n",
        "\n",
        "\n",
        "with open(final_json_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump([chunk.model_dump() for chunk in summarized_chunks], f, ensure_ascii=False, indent=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEfXe-FJX0GS"
      },
      "source": [
        "## GENERATE VM CHUNKS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE3L-RZmyNqY",
        "outputId": "2bfb18ab-d61a-4575-ddd0-a94e06d4444b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Summarizing chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92/92 [10:34<00:00,  6.90s/it]\n"
          ]
        }
      ],
      "source": [
        "client = from_openai(OpenAI(api_key=OPENAI_API_KEY))\n",
        "\n",
        "md_path = \"/data/VM_manual/ISTRUZIONE_OPERATIVA_CREAZIONE_VM_CLOUD_INSIEL_REV_00_cleaned.md\"\n",
        "document_name = \"ISTRUZIONE OPERATIVA CREAZIONE VM CLOUD INSIEL REV 00\"\n",
        "figure_metadata_path = '/data/VM_manual/figure_metadata.json'\n",
        "final_json_path = '/data/VM_manual/final_VM_chunks.json'\n",
        "\n",
        "with open(md_path, 'r', encoding='utf-8') as f:\n",
        "    markdown_text = f.read()\n",
        "\n",
        "lines = markdown_text.splitlines()\n",
        "line_to_page = build_line_to_page_map(lines)\n",
        "stripped_lines = strip_page_markers(lines)\n",
        "stripped_markdown = \"\\n\".join(stripped_lines)\n",
        "\n",
        "parsed_headings = parse_heading_structure(markdown_text)\n",
        "\n",
        "parsed_headings = parse_heading_structure(markdown_text)\n",
        "headings_with_parents = resolve_parent_topics(parsed_headings)\n",
        "\n",
        "\n",
        "raw_chunks = generate_semantic_chunks_from_headings(\n",
        "    stripped_markdown,\n",
        "    headings_with_parents,\n",
        "    document_name,\n",
        "    line_to_page=line_to_page,\n",
        "    original_lines=lines \n",
        ")\n",
        "\n",
        "\n",
        "semantic_chunks = [SemanticChunk(**chunk) for chunk in raw_chunks]\n",
        "deduped_chunks = deduplicate_by_section_number(semantic_chunks)\n",
        "\n",
        "with open(figure_metadata_path, 'r') as f:\n",
        "    figure_metadata_dict = json.load(f)\n",
        "\n",
        "semantic_chunks_with_figures = attach_figures_to_chunks(deduped_chunks, figure_metadata_dict)\n",
        "\n",
        "summarized_chunks = summarize_chunks(semantic_chunks_with_figures)\n",
        "\n",
        "with open(final_json_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump([chunk.model_dump() for chunk in summarized_chunks], f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSCSaz4GlR_t"
      },
      "source": [
        "## Let's evaluate our new JSONs to make sure they make sense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8Z451LlInML2"
      },
      "outputs": [],
      "source": [
        "def check_summary_quality(chunks, min_word_count=8):\n",
        "    empty = [c for c in chunks if not c.chunk_summary.strip()]\n",
        "    short = [c for c in chunks if len(c.chunk_summary.split()) < min_word_count]\n",
        "\n",
        "    return {\n",
        "        \"empty_summaries\": len(empty),\n",
        "        \"short_summaries\": len(short),\n",
        "        \"examples\": [(c.section_number, c.chunk_summary) for c in short[:3]]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "30QActUNnNbf"
      },
      "outputs": [],
      "source": [
        "def check_figure_alignment(chunks):\n",
        "    misaligned = [c for c in chunks if \"Figura\" in c.content and not c.figures]\n",
        "    return {\n",
        "        \"missing_figure_links\": len(misaligned),\n",
        "        \"examples\": [(c.section_number, c.content[:200]) for c in misaligned[:3]]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "g2jUW4zLnQCu"
      },
      "outputs": [],
      "source": [
        "def estimate_token_count(text):\n",
        "    return int(len(text) / 4)\n",
        "\n",
        "def check_embedding_compatibility(chunks, max_tokens=8192):\n",
        "    too_large = [c for c in chunks if estimate_token_count(c.content) > max_tokens]\n",
        "    return {\n",
        "        \"too_large_chunks\": len(too_large),\n",
        "        \"examples\": [(c.section_number, estimate_token_count(c.content)) for c in too_large[:3]]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "W3rBsK3QnQD_"
      },
      "outputs": [],
      "source": [
        "def check_section_numbers(chunks):\n",
        "    section_nums = [c.section_number for c in chunks]\n",
        "    seen = set()\n",
        "    duplicates = []\n",
        "\n",
        "    for sec in section_nums:\n",
        "        if sec in seen:\n",
        "            duplicates.append(sec)\n",
        "        seen.add(sec)\n",
        "\n",
        "    def to_tuple(s):\n",
        "        return tuple(int(x) for x in s.strip('.').split('.'))\n",
        "\n",
        "    sorted_sections = sorted(section_nums, key=to_tuple)\n",
        "    out_of_order = [s for s, t in zip(section_nums, sorted_sections) if s != t]\n",
        "\n",
        "    return {\n",
        "        \"duplicates\": duplicates,\n",
        "        \"out_of_order\": out_of_order[:5]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "9_52DVuMp8LZ"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "openai = OpenAI(api_key=OPENAI_API_KEY))\n",
        "\n",
        "def test_retrieval_qa(chunk: SemanticChunk, question: str) -> str:\n",
        "    system_prompt = \"You are an expert assistant helping with technical documentation.\"\n",
        "    context = f\"Use the following document section:\\n\\n{chunk.content}\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"{context}\\n\\nQuestion: {question}\"}\n",
        "    ]\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-4-turbo\",\n",
        "        messages=messages,\n",
        "        temperature=0.0\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NFW7-oyYnlZu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from typing import List\n",
        "\n",
        "def load_chunks_from_json(path: str) -> List[SemanticChunk]:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw_data = json.load(f)\n",
        "\n",
        "    return [SemanticChunk(**chunk) for chunk in raw_data]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p38dC8aeRYfB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZDgN_U7ilYCw"
      },
      "outputs": [],
      "source": [
        "def evaluate_chunks_from_valid(chunks):\n",
        "    results = {}\n",
        "\n",
        "    results.update(check_summary_quality(chunks))\n",
        "    results.update(check_figure_alignment(chunks))\n",
        "    results.update(check_section_numbers(chunks))\n",
        "    results.update(check_embedding_compatibility(chunks))\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnjKJ_vVlZ5b",
        "outputId": "d56d4e61-71cc-47ce-a717-ccf3df62bccd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'empty_summaries': 0, 'short_summaries': 0, 'examples': [], 'missing_figure_links': 0, 'duplicates': [], 'out_of_order': [], 'too_large_chunks': 0}\n"
          ]
        }
      ],
      "source": [
        "chunks = load_chunks_from_json(\"/content/drive/MyDrive/rag_comparison/data/wifi_manual/final_wifi_chunks.json\")\n",
        "results = evaluate_chunks_from_valid(chunks)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7WIGfErnubo",
        "outputId": "c4ffa1cd-3cc8-428c-defc-33ece6e67fdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'empty_summaries': 0, 'short_summaries': 0, 'examples': [], 'missing_figure_links': 0, 'duplicates': [], 'out_of_order': ['5.4.1.1.6', '5.5.1.', '5.5.1.1.', '5.5.1.2.', '5.5.1.3.'], 'too_large_chunks': 0}\n"
          ]
        }
      ],
      "source": [
        "chunks = load_chunks_from_json(\"/content/drive/MyDrive/rag_comparison/data/VM_manual/final_VM_chunks.json\")\n",
        "results = evaluate_chunks_from_valid(chunks)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mCXHWIg8o7LX"
      },
      "outputs": [],
      "source": [
        "def print_section_order(chunks):\n",
        "    def to_tuple(sec):\n",
        "        return tuple(int(x) for x in sec.strip(\".\").split(\".\") if x.isdigit())\n",
        "\n",
        "    sorted_chunks = sorted(chunks, key=lambda c: to_tuple(c.section_number))\n",
        "\n",
        "    for chunk in sorted_chunks:\n",
        "        print(chunk.section_number)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxmhDbCypKdV",
        "outputId": "edbe8f9e-e220-4281-f735-862dcbf5a06b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "5.1\n",
            "5.1.1\n",
            "5.1.2\n",
            "5.1.3\n",
            "5.1.3.1\n",
            "5.1.3.1.1\n",
            "5.1.3.1.2\n",
            "5.1.3.1.3\n",
            "5.1.3.1.4\n",
            "5.1.3.2\n",
            "5.1.3.2.1\n",
            "5.1.3.2.2\n",
            "5.1.3.2.3\n",
            "5.1.3.2.4\n",
            "5.1.3.3\n",
            "5.1.3.3.1\n",
            "5.1.3.4\n",
            "5.1.3.4.1\n",
            "5.1.3.5\n",
            "5.1.3.5.1\n",
            "5.1.3.6\n",
            "5.1.3.7\n",
            "5.1.3.8\n",
            "5.1.3.9\n",
            "5.1.3.10.\n",
            "5.1.3.11.\n",
            "5.1.3.12.\n",
            "5.1.4\n",
            "5.2\n",
            "5.2.1\n",
            "5.2.2\n",
            "5.2.2.1\n",
            "5.2.2.2\n",
            "6\n",
            "7\n",
            "7.1\n",
            "7.2\n",
            "7.3\n",
            "8\n",
            "9\n",
            "10\n"
          ]
        }
      ],
      "source": [
        "wifi_chunks = load_chunks_from_json(\"/content/drive/MyDrive/rag_comparison/data/wifi_manual/final_wifi_chunks.json\")\n",
        "print_section_order(wifi_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPVW2_rho9Qe",
        "outputId": "c7ff9913-18c8-49fa-efa5-2e599f7e3e57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "5.1\n",
            "5.1.1\n",
            "5.1.2\n",
            "5.1.3\n",
            "5.2\n",
            "5.2.1\n",
            "5.2.2\n",
            "5.2.2.1\n",
            "5.2.2.2\n",
            "5.3\n",
            "5.3.1\n",
            "5.3.1.1\n",
            "5.3.1.2\n",
            "5.3.1.3\n",
            "5.3.2\n",
            "5.3.2.1\n",
            "5.3.2.1.1\n",
            "5.3.2.1.2\n",
            "5.3.2.1.3\n",
            "5.3.2.1.3.1.\n",
            "5.3.2.1.4\n",
            "5.3.2.1.4.1.\n",
            "5.3.2.1.4.2.\n",
            "5.3.2.1.4.3.\n",
            "5.3.2.1.4.4.\n",
            "5.3.2.1.4.5.\n",
            "5.4\n",
            "5.4.1.1.1\n",
            "5.4.1.1.2\n",
            "5.4.1.1.2.1.\n",
            "5.4.1.1.2.2.\n",
            "5.4.1.1.3\n",
            "5.4.1.1.3.1.\n",
            "5.4.1.1.4\n",
            "5.4.1.1.5\n",
            "5.4.1.1.5.1.\n",
            "5.4.1.1.5.2.\n",
            "5.4.1.1.6\n",
            "5.4.1.1.7\n",
            "5.4.1.1.7.1.\n",
            "5.4.1.1.7.2.\n",
            "5.4.1.1.8\n",
            "5.4.1.1.8.1.\n",
            "5.4.1.1.9\n",
            "5.4.1.1.9.1.\n",
            "5.4.1.1.9.2.\n",
            "5.4.1.1.9.3.\n",
            "5.4.1.1.9.4.\n",
            "5.5\n",
            "5.5.1.\n",
            "5.5.1.1.\n",
            "5.5.1.2.\n",
            "5.5.1.3.\n",
            "5.5.1.4.\n",
            "5.5.1.5.\n",
            "5.5.1.6.\n",
            "5.5.1.7.\n",
            "5.5.1.8.\n",
            "5.5.2.\n",
            "5.5.2.1.\n",
            "5.5.2.2.\n",
            "5.5.2.3.\n",
            "5.5.3.\n",
            "5.5.3.1.\n",
            "5.5.3.1.1.\n",
            "5.5.3.1.2.\n",
            "5.5.3.2.\n",
            "5.5.3.3.\n",
            "5.5.3.4.\n",
            "5.5.3.4.1.\n",
            "5.5.3.4.2.\n",
            "5.5.3.5.\n",
            "5.5.3.5.1.\n",
            "6.\n",
            "6.1.\n",
            "6.2.\n",
            "7.\n",
            "7.1.\n",
            "7.2.\n",
            "8.\n",
            "9.\n",
            "9.1.\n",
            "9.2.\n",
            "9.3.\n",
            "10.\n",
            "11.\n",
            "12.\n"
          ]
        }
      ],
      "source": [
        "vm_chunks = load_chunks_from_json(\"/content/drive/MyDrive/rag_comparison/data/VM_manual/final_VM_chunks.json\")\n",
        "print_section_order(vm_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "3diNsi8Kpmim"
      },
      "outputs": [],
      "source": [
        "def normalize_section_number(sec: str) -> str:\n",
        "    return sec.strip().rstrip(\".\")\n",
        "from collections import defaultdict\n",
        "\n",
        "def group_chunks_by_section(chunks):\n",
        "    grouped = defaultdict(list)\n",
        "    for c in chunks:\n",
        "        key = normalize_section_number(c.section_number)\n",
        "        grouped[key].append(c)\n",
        "    return {k: v for k, v in grouped.items() if len(v) > 1}  # only duplicates\n",
        "def compare_duplicate_chunks(grouped):\n",
        "    for section, versions in grouped.items():\n",
        "        print(f\"\\nðŸ” Duplicate Section: {section} (x{len(versions)})\")\n",
        "        for i, c in enumerate(versions):\n",
        "            print(f\"\\nâ€” Version {i+1} â€”\")\n",
        "            print(f\"Title: {c.section_title}\")\n",
        "            print(f\"Summary: {c.chunk_summary}\")\n",
        "            print(f\"Figures: {[f.figure_id for f in c.figures]}\")\n",
        "            print(f\"Content Preview:\\n{c.content[:300]}...\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 387,
      "metadata": {
        "id": "qlweOmCduGie"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def normalize_section_number(sec: str) -> str:\n",
        "    return sec.strip().rstrip(\".\")\n",
        "\n",
        "def group_duplicates(chunks):\n",
        "    grouped = defaultdict(list)\n",
        "    for c in chunks:\n",
        "        norm = normalize_section_number(c.section_number)\n",
        "        grouped[norm].append(c)\n",
        "    return {k: v for k, v in grouped.items() if len(v) > 1}\n",
        "\n",
        "import difflib\n",
        "\n",
        "def analyze_duplicates(grouped):\n",
        "    comparison_report = []\n",
        "\n",
        "    for section, versions in grouped.items():\n",
        "        if len(versions) != 2:\n",
        "            continue\n",
        "\n",
        "        v1, v2 = versions\n",
        "\n",
        "        v1_words = len(v1.content.split())\n",
        "        v2_words = len(v2.content.split())\n",
        "\n",
        "        v1_figs = len(v1.figures)\n",
        "        v2_figs = len(v2.figures)\n",
        "\n",
        "        # summary_ratio = difflib.SequenceMatcher(None, v1.chunk_summary, v2.chunk_summary).ratio()\n",
        "        content_ratio = difflib.SequenceMatcher(None, v1.content, v2.content).ratio()\n",
        "\n",
        "        comparison_report.append({\n",
        "            \"section\": section,\n",
        "            \"v1_words\": v1_words,\n",
        "            \"v2_words\": v2_words,\n",
        "            \"v1_figures\": v1_figs,\n",
        "            \"v2_figures\": v2_figs,\n",
        "            # \"summary_similarity\": round(summary_ratio, 2),\n",
        "            \"content_similarity\": round(content_ratio, 2),\n",
        "            \"v1_preview\": v1.content[:150].strip(),\n",
        "            \"v2_preview\": v2.content[:150].strip()\n",
        "        })\n",
        "\n",
        "    return comparison_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "yrGmuxD2qi0R"
      },
      "outputs": [],
      "source": [
        "duplicates = group_chunks_by_section(wifi_chunks)\n",
        "\n",
        "# Compare side-by-side\n",
        "compare_duplicate_chunks(duplicates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "5UxYziMEprbw"
      },
      "outputs": [],
      "source": [
        "duplicates = group_chunks_by_section(vm_chunks)\n",
        "\n",
        "# Compare side-by-side\n",
        "compare_duplicate_chunks(duplicates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 381,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mzjpmDypuE_y",
        "outputId": "9aa77159-6c50-4302-a9db-657003ffc95d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 39,\n  \"fields\": [\n    {\n      \"column\": \"section\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 39,\n        \"samples\": [\n          \"5.5.3.4.2\",\n          \"5.5.3\",\n          \"5.5.1.6\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"v1_words\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7,\n        \"min\": 3,\n        \"max\": 52,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          5,\n          7,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"v2_words\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 124,\n        \"min\": 4,\n        \"max\": 775,\n        \"num_unique_values\": 23,\n        \"samples\": [\n          13,\n          30,\n          775\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"v1_figures\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"v2_figures\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 0,\n        \"max\": 53,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          53\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.35678747307699776,\n        \"min\": 0.01,\n        \"max\": 1.0,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          0.28\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"v1_preview\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 39,\n        \"samples\": [\n          \"#### 5.5.3.4.2. AGGIORNAMENTO e patching\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"v2_preview\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 39,\n        \"samples\": [\n          \"#### 5.5.3.4.2. AGGIORNAMENTO E PATCHING\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-c86e3f8f-4737-4a70-9b64-a5bc2cb68f6f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>section</th>\n",
              "      <th>v1_words</th>\n",
              "      <th>v2_words</th>\n",
              "      <th>v1_figures</th>\n",
              "      <th>v2_figures</th>\n",
              "      <th>content_similarity</th>\n",
              "      <th>v1_preview</th>\n",
              "      <th>v2_preview</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>12</td>\n",
              "      <td>10</td>\n",
              "      <td>775</td>\n",
              "      <td>0</td>\n",
              "      <td>53</td>\n",
              "      <td>0.01</td>\n",
              "      <td>## 12. INDICE DELLE FIGURE\\n\\n\\n\\n\\n*Pagina 5 ...</td>\n",
              "      <td># 12. INDICE DELLE FIGURE\\n\\nFIGURA 1 - VMWARE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>6.1</td>\n",
              "      <td>5</td>\n",
              "      <td>160</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>### 6.1. SEGMENTI DI RETE</td>\n",
              "      <td>### 6.1. SEGMENTI DI RETE\\n\\nNello IaaS INSIEL...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.4.1.1.6</td>\n",
              "      <td>52</td>\n",
              "      <td>96</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.03</td>\n",
              "      <td>##### 5.4.1.1.6 NAT47\\n##### 5.4.1.1.7 DNS.......</td>\n",
              "      <td>##### 5.4.1.1.6 NAT\\n\\n\\n\\n\\n\\n\\nNessuna regol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>7.2</td>\n",
              "      <td>4</td>\n",
              "      <td>74</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.08</td>\n",
              "      <td>### 7.2. NOMI GRUPPI</td>\n",
              "      <td>## 7.2. NOMI GRUPPI\\n\\nPer i nomi dei gruppi v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>5.5.1.6</td>\n",
              "      <td>6</td>\n",
              "      <td>72</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.15</td>\n",
              "      <td>### 5.5.1.6. ESPANSIONE DISCO DI SISTEMA</td>\n",
              "      <td>### 5.5.1.6. ESPANSIONE DISCO DI SISTEMA\\nIn f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>6.2</td>\n",
              "      <td>10</td>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.27</td>\n",
              "      <td>### 6.2. RANGE DI IP DEDICATI PER TIPOLOGIA DI...</td>\n",
              "      <td>### 6.2. RANGE DI IP DEDICATI PER TIPOLOGIA DI...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>53</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.18</td>\n",
              "      <td>## 8. GESTIONE DELLA DOCUMENTAZIONE</td>\n",
              "      <td>## 8. GESTIONE DELLA DOCUMENTAZIONE\\n\\nNel pre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5.5.1.5</td>\n",
              "      <td>4</td>\n",
              "      <td>52</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>### 5.5.1.5. DISCHI AGGIUNTIVI</td>\n",
              "      <td>### 5.5.1.5. DISCHI AGGIUNTIVI\\nÃˆ possibile ch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>5.5.3.1</td>\n",
              "      <td>5</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.28</td>\n",
              "      <td>### 5.5.3.1. MODIFICA NOME HOST</td>\n",
              "      <td>### 5.5.3.1. MODIFICA NOME HOST\\nIl nome host ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>5.5.1.7</td>\n",
              "      <td>5</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.30</td>\n",
              "      <td>### 5.5.1.7. CAMBIO DENOMINAZIONE VM</td>\n",
              "      <td>### 5.5.1.7. CAMBIO DENOMINAZIONE VM\\nDopo la ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>5.5.1.8</td>\n",
              "      <td>6</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.30</td>\n",
              "      <td>### 5.5.1.8. PRIMO ACCESSO ALLA VM</td>\n",
              "      <td>### 5.5.1.8. PRIMO ACCESSO ALLA VM\\nInfine, Ã¨ ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.5.1.1</td>\n",
              "      <td>6</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.35</td>\n",
              "      <td>### 5.5.1.1. ACCESSO A CLOUD MANAGER</td>\n",
              "      <td>#### 5.5.1.1. ACCESSO A CLOUD MANAGER\\nPer l'a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5.5.1.4</td>\n",
              "      <td>4</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.28</td>\n",
              "      <td>### 5.5.1.4. VERIFICA CAPIENZA</td>\n",
              "      <td>### 5.5.1.4. VERIFICA CAPIENZA\\nÃˆ possibile ch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.5.1.3</td>\n",
              "      <td>4</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.30</td>\n",
              "      <td>### 5.5.1.3. SELEZIONE T-SHIRT</td>\n",
              "      <td>#### 5.5.1.3. SELEZIONE T-SHIRT\\nLa selezione ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>5.5.2.1</td>\n",
              "      <td>5</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.28</td>\n",
              "      <td>### 5.5.2.1. ACCESSO A NSX</td>\n",
              "      <td>### 5.5.2.1. ACCESSO A NSX\\nPer l'accesso al c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>5.5.2</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.26</td>\n",
              "      <td>## 5.5.2. REGOLE FIREWALL</td>\n",
              "      <td>## 5.5.2. REGOLE FIREWALL\\nDopo la creazione d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>7.1</td>\n",
              "      <td>5</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.33</td>\n",
              "      <td>### 7.1. NOMI MACCHINE VIRTUALI</td>\n",
              "      <td>## 7.1. NOMI MACCHINE VIRTUALI\\n\\nAnalogamente...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.5.1.2</td>\n",
              "      <td>5</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.49</td>\n",
              "      <td>### 5.5.1.2. SELEZIONE SISTEMA OPERATIVO</td>\n",
              "      <td>#### 5.5.1.2. SELEZIONE SISTEMA OPERATIVO\\nLa ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>9.3</td>\n",
              "      <td>4</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>### 9.3. VMWARE ARIA</td>\n",
              "      <td>### 9.3. VMWARE ARIA\\n- HTTPS://DOCS.VMWARE.CO...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>5.5.3.5.1</td>\n",
              "      <td>7</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.73</td>\n",
              "      <td>#### 5.5.3.5.1. VERIFICA FUNZIONAMENTO SU CONS...</td>\n",
              "      <td>#### 5.5.3.5.1. VERIFICA FUNZIONAMENTO SU CONS...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.69</td>\n",
              "      <td>## 7. APPENDICE 2 â€“ NAMING CONVENTION</td>\n",
              "      <td>## 7. APPENDICE 2 â€“ NAMING CONVENTION\\n\\n\\n\\n*...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>6</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>## 6. APPENDICE 1 â€“ CRITERI DI ASSEGNAZIONE DE...</td>\n",
              "      <td>## 6. APPENDICE 1 â€“ CRITERI DI ASSEGNAZIONE DE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>9.2</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.26</td>\n",
              "      <td>### 9.2. VMWARE NSX</td>\n",
              "      <td>### 9.2. VMWARE NSX\\n- HTTPS://WWW.VMWARE.COM/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>9.1</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.28</td>\n",
              "      <td>### 9.1. VMWARE VSPHERE</td>\n",
              "      <td>### 9.1. VMWARE VSPHERE\\n- HTTPS://WWW.VMWARE....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>5.5.3.1.2</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>#### 5.5.3.1.2. Linux</td>\n",
              "      <td>#### 5.5.3.1.2. LINUX\\n\\n\\n\\n*Pagina 60 di 66 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>11</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.84</td>\n",
              "      <td>## 11. ALLEGATI (SE PRESENTI)</td>\n",
              "      <td>## 11. ALLEGATI (SE PRESENTI)\\n\\nâ€¢ Nessuno</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>5.5.3.4.1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>#### 5.5.3.4.1. VERIFICA SU CONSOLE WSUS</td>\n",
              "      <td>#### 5.5.3.4.1. VERIFICA SU CONSOLE WSUS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.45</td>\n",
              "      <td>## 10. DOCUMENTI COLLEGATI</td>\n",
              "      <td>## 10. DOCUMENTI COLLEGATI\\n\\nâ€¢ HTTPS://WIKI.A...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>## 9. RIFERIMENTI (NORMATIVI E BIBLIOGRAFICI)</td>\n",
              "      <td>## 9. RIFERIMENTI (NORMATIVI E BIBLIOGRAFICI)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>5.5.3.3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>### 5.5.3.3. ESPANSIONE DISCO DI SISTEMA</td>\n",
              "      <td>### 5.5.3.3. ESPANSIONE DISCO DI SISTEMA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>5.5.3.1.1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.53</td>\n",
              "      <td>#### 5.5.3.1.1. Microsoft Windows Server</td>\n",
              "      <td>#### 5.5.3.1.1. MICROSOFT WINDOWS SERVER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>5.5.3.2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>### 5.5.3.2. AGGIUNTA A DOMINIO</td>\n",
              "      <td>### 5.5.3.2. AGGIUNTA A DOMINIO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>5.5.2.3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>### 5.5.2.3. PROBLEMI DI RETE</td>\n",
              "      <td>### 5.5.2.3. PROBLEMI DI RETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>5.5.3.4.2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.78</td>\n",
              "      <td>#### 5.5.3.4.2. AGGIORNAMENTO e patching</td>\n",
              "      <td>#### 5.5.3.4.2. AGGIORNAMENTO E PATCHING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>5.5.3.4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>### 5.5.3.4. AGGIORNAMENTO SISTEMA OPERATIVO</td>\n",
              "      <td>### 5.5.3.4. AGGIORNAMENTO SISTEMA OPERATIVO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.5.1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.98</td>\n",
              "      <td>## 5.5.1. CREAZIONE VM</td>\n",
              "      <td>### 5.5.1. CREAZIONE VM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5.5.3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>## 5.5.3. CONFIGURAZIONE VM</td>\n",
              "      <td>## 5.5.3. CONFIGURAZIONE VM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>5.5.2.2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>### 5.5.2.2. CONFIGURAZIONE REGOLE</td>\n",
              "      <td>### 5.5.2.2. CONFIGURAZIONE REGOLE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>5.5.3.5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>### 5.5.3.5. INSTALLAZIONE ANTIVIRUS</td>\n",
              "      <td>### 5.5.3.5. INSTALLAZIONE ANTIVIRUS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c86e3f8f-4737-4a70-9b64-a5bc2cb68f6f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c86e3f8f-4737-4a70-9b64-a5bc2cb68f6f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c86e3f8f-4737-4a70-9b64-a5bc2cb68f6f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6bba4b86-ea63-4d85-ba5e-0881595b95a5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6bba4b86-ea63-4d85-ba5e-0881595b95a5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6bba4b86-ea63-4d85-ba5e-0881595b95a5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "      section  v1_words  v2_words  v1_figures  v2_figures  content_similarity  \\\n",
              "38         12        10       775           0          53                0.01   \n",
              "26        6.1         5       160           0           0                0.05   \n",
              "0   5.4.1.1.6        52        96           0           2                0.03   \n",
              "30        7.2         4        74           0           0                0.08   \n",
              "7     5.5.1.6         6        72           0           0                0.15   \n",
              "27        6.2        10        58           0           0                0.27   \n",
              "31          8         5        53           0           0                0.18   \n",
              "6     5.5.1.5         4        52           0           0                0.17   \n",
              "15    5.5.3.1         5        32           0           0                0.28   \n",
              "8     5.5.1.7         5        30           0           0                0.30   \n",
              "9     5.5.1.8         6        27           0           0                0.30   \n",
              "2     5.5.1.1         6        27           0           0                0.35   \n",
              "5     5.5.1.4         4        27           0           0                0.28   \n",
              "4     5.5.1.3         4        27           0           0                0.30   \n",
              "11    5.5.2.1         5        26           0           0                0.28   \n",
              "10      5.5.2         4        25           0           0                0.26   \n",
              "29        7.1         5        23           0           0                0.33   \n",
              "3     5.5.1.2         5        20           0           0                0.49   \n",
              "35        9.3         4        13           0           0                0.14   \n",
              "24  5.5.3.5.1         7        13           0           0                0.73   \n",
              "28          7         7        12           0           0                0.69   \n",
              "25          6        11        11           0           0                1.00   \n",
              "34        9.2         4         8           0           0                0.26   \n",
              "33        9.1         4         8           0           0                0.28   \n",
              "17  5.5.3.1.2         3         8           0           0                0.50   \n",
              "37         11         5         7           0           0                0.84   \n",
              "21  5.5.3.4.1         6         6           0           0                1.00   \n",
              "36         10         4         6           0           0                0.45   \n",
              "32          9         6         6           0           0                1.00   \n",
              "19    5.5.3.3         6         6           0           0                1.00   \n",
              "16  5.5.3.1.1         5         5           0           0                0.53   \n",
              "18    5.5.3.2         5         5           0           0                1.00   \n",
              "13    5.5.2.3         5         5           0           0                1.00   \n",
              "22  5.5.3.4.2         5         5           0           0                0.78   \n",
              "20    5.5.3.4         5         5           0           0                1.00   \n",
              "1       5.5.1         4         4           0           0                0.98   \n",
              "14      5.5.3         4         4           0           0                1.00   \n",
              "12    5.5.2.2         4         4           0           0                1.00   \n",
              "23    5.5.3.5         4         4           0           0                1.00   \n",
              "\n",
              "                                           v1_preview  \\\n",
              "38  ## 12. INDICE DELLE FIGURE\\n\\n\\n\\n\\n*Pagina 5 ...   \n",
              "26                          ### 6.1. SEGMENTI DI RETE   \n",
              "0   ##### 5.4.1.1.6 NAT47\\n##### 5.4.1.1.7 DNS.......   \n",
              "30                               ### 7.2. NOMI GRUPPI   \n",
              "7            ### 5.5.1.6. ESPANSIONE DISCO DI SISTEMA   \n",
              "27  ### 6.2. RANGE DI IP DEDICATI PER TIPOLOGIA DI...   \n",
              "31                ## 8. GESTIONE DELLA DOCUMENTAZIONE   \n",
              "6                      ### 5.5.1.5. DISCHI AGGIUNTIVI   \n",
              "15                    ### 5.5.3.1. MODIFICA NOME HOST   \n",
              "8                ### 5.5.1.7. CAMBIO DENOMINAZIONE VM   \n",
              "9                  ### 5.5.1.8. PRIMO ACCESSO ALLA VM   \n",
              "2                ### 5.5.1.1. ACCESSO A CLOUD MANAGER   \n",
              "5                      ### 5.5.1.4. VERIFICA CAPIENZA   \n",
              "4                      ### 5.5.1.3. SELEZIONE T-SHIRT   \n",
              "11                         ### 5.5.2.1. ACCESSO A NSX   \n",
              "10                          ## 5.5.2. REGOLE FIREWALL   \n",
              "29                    ### 7.1. NOMI MACCHINE VIRTUALI   \n",
              "3            ### 5.5.1.2. SELEZIONE SISTEMA OPERATIVO   \n",
              "35                               ### 9.3. VMWARE ARIA   \n",
              "24  #### 5.5.3.5.1. VERIFICA FUNZIONAMENTO SU CONS...   \n",
              "28              ## 7. APPENDICE 2 â€“ NAMING CONVENTION   \n",
              "25  ## 6. APPENDICE 1 â€“ CRITERI DI ASSEGNAZIONE DE...   \n",
              "34                                ### 9.2. VMWARE NSX   \n",
              "33                            ### 9.1. VMWARE VSPHERE   \n",
              "17                              #### 5.5.3.1.2. Linux   \n",
              "37                      ## 11. ALLEGATI (SE PRESENTI)   \n",
              "21           #### 5.5.3.4.1. VERIFICA SU CONSOLE WSUS   \n",
              "36                         ## 10. DOCUMENTI COLLEGATI   \n",
              "32      ## 9. RIFERIMENTI (NORMATIVI E BIBLIOGRAFICI)   \n",
              "19           ### 5.5.3.3. ESPANSIONE DISCO DI SISTEMA   \n",
              "16           #### 5.5.3.1.1. Microsoft Windows Server   \n",
              "18                    ### 5.5.3.2. AGGIUNTA A DOMINIO   \n",
              "13                      ### 5.5.2.3. PROBLEMI DI RETE   \n",
              "22           #### 5.5.3.4.2. AGGIORNAMENTO e patching   \n",
              "20       ### 5.5.3.4. AGGIORNAMENTO SISTEMA OPERATIVO   \n",
              "1                              ## 5.5.1. CREAZIONE VM   \n",
              "14                        ## 5.5.3. CONFIGURAZIONE VM   \n",
              "12                 ### 5.5.2.2. CONFIGURAZIONE REGOLE   \n",
              "23               ### 5.5.3.5. INSTALLAZIONE ANTIVIRUS   \n",
              "\n",
              "                                           v2_preview  \n",
              "38  # 12. INDICE DELLE FIGURE\\n\\nFIGURA 1 - VMWARE...  \n",
              "26  ### 6.1. SEGMENTI DI RETE\\n\\nNello IaaS INSIEL...  \n",
              "0   ##### 5.4.1.1.6 NAT\\n\\n\\n\\n\\n\\n\\nNessuna regol...  \n",
              "30  ## 7.2. NOMI GRUPPI\\n\\nPer i nomi dei gruppi v...  \n",
              "7   ### 5.5.1.6. ESPANSIONE DISCO DI SISTEMA\\nIn f...  \n",
              "27  ### 6.2. RANGE DI IP DEDICATI PER TIPOLOGIA DI...  \n",
              "31  ## 8. GESTIONE DELLA DOCUMENTAZIONE\\n\\nNel pre...  \n",
              "6   ### 5.5.1.5. DISCHI AGGIUNTIVI\\nÃˆ possibile ch...  \n",
              "15  ### 5.5.3.1. MODIFICA NOME HOST\\nIl nome host ...  \n",
              "8   ### 5.5.1.7. CAMBIO DENOMINAZIONE VM\\nDopo la ...  \n",
              "9   ### 5.5.1.8. PRIMO ACCESSO ALLA VM\\nInfine, Ã¨ ...  \n",
              "2   #### 5.5.1.1. ACCESSO A CLOUD MANAGER\\nPer l'a...  \n",
              "5   ### 5.5.1.4. VERIFICA CAPIENZA\\nÃˆ possibile ch...  \n",
              "4   #### 5.5.1.3. SELEZIONE T-SHIRT\\nLa selezione ...  \n",
              "11  ### 5.5.2.1. ACCESSO A NSX\\nPer l'accesso al c...  \n",
              "10  ## 5.5.2. REGOLE FIREWALL\\nDopo la creazione d...  \n",
              "29  ## 7.1. NOMI MACCHINE VIRTUALI\\n\\nAnalogamente...  \n",
              "3   #### 5.5.1.2. SELEZIONE SISTEMA OPERATIVO\\nLa ...  \n",
              "35  ### 9.3. VMWARE ARIA\\n- HTTPS://DOCS.VMWARE.CO...  \n",
              "24  #### 5.5.3.5.1. VERIFICA FUNZIONAMENTO SU CONS...  \n",
              "28  ## 7. APPENDICE 2 â€“ NAMING CONVENTION\\n\\n\\n\\n*...  \n",
              "25  ## 6. APPENDICE 1 â€“ CRITERI DI ASSEGNAZIONE DE...  \n",
              "34  ### 9.2. VMWARE NSX\\n- HTTPS://WWW.VMWARE.COM/...  \n",
              "33  ### 9.1. VMWARE VSPHERE\\n- HTTPS://WWW.VMWARE....  \n",
              "17  #### 5.5.3.1.2. LINUX\\n\\n\\n\\n*Pagina 60 di 66 ...  \n",
              "37         ## 11. ALLEGATI (SE PRESENTI)\\n\\nâ€¢ Nessuno  \n",
              "21           #### 5.5.3.4.1. VERIFICA SU CONSOLE WSUS  \n",
              "36  ## 10. DOCUMENTI COLLEGATI\\n\\nâ€¢ HTTPS://WIKI.A...  \n",
              "32      ## 9. RIFERIMENTI (NORMATIVI E BIBLIOGRAFICI)  \n",
              "19           ### 5.5.3.3. ESPANSIONE DISCO DI SISTEMA  \n",
              "16           #### 5.5.3.1.1. MICROSOFT WINDOWS SERVER  \n",
              "18                    ### 5.5.3.2. AGGIUNTA A DOMINIO  \n",
              "13                      ### 5.5.2.3. PROBLEMI DI RETE  \n",
              "22           #### 5.5.3.4.2. AGGIORNAMENTO E PATCHING  \n",
              "20       ### 5.5.3.4. AGGIORNAMENTO SISTEMA OPERATIVO  \n",
              "1                             ### 5.5.1. CREAZIONE VM  \n",
              "14                        ## 5.5.3. CONFIGURAZIONE VM  \n",
              "12                 ### 5.5.2.2. CONFIGURAZIONE REGOLE  \n",
              "23               ### 5.5.3.5. INSTALLAZIONE ANTIVIRUS  "
            ]
          },
          "execution_count": 381,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "groups = group_duplicates(vm_chunks)\n",
        "comparison_report = analyze_duplicates(groups)\n",
        "df = pd.DataFrame(comparison_report)\n",
        "df.sort_values(\"v2_words\", ascending=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2n30ziovC_R",
        "outputId": "c3328c31-33dc-4120-c9cc-39269c1541cc"
      },
      "outputs": [],
      "source": [
        "# for c in summarized_chunks:\n",
        "#     c.section_number = normalize_section_number(c.section_number)\n",
        "\n",
        "deduped_chunks = deduplicate_by_section_number(vm_chunks)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/rag_comparison/data/VM_manual/final_VM_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump([c.model_dump() for c in deduped_chunks], f, ensure_ascii=False, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "oknyaMpYu7OA"
      },
      "outputs": [],
      "source": [
        "def extract_used_figures(chunks: List[SemanticChunk]) -> set:\n",
        "    used = set()\n",
        "    for chunk in chunks:\n",
        "        for fig in chunk.figures:\n",
        "            used.add(fig[\"figure_id\"] if isinstance(fig, dict) else fig.figure_id)\n",
        "    return used\n",
        "def compare_figures(fig_metadata, chunks):\n",
        "    defined = {fig[\"figure_id\"] for fig in fig_metadata}\n",
        "    used = extract_used_figures(chunks)\n",
        "\n",
        "    missing_from_chunks = defined - used\n",
        "    unknown_figures = used - defined\n",
        "\n",
        "    return {\n",
        "        \"defined_total\": len(defined),\n",
        "        \"used_total\": len(used),\n",
        "        \"unused_figures\": sorted(missing_from_chunks),\n",
        "        \"invalid_figures\": sorted(unknown_figures)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qvD6wxP3wdCU"
      },
      "outputs": [],
      "source": [
        "def load_figure_metadata_from_dict(path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "\n",
        "    if not isinstance(data, dict):\n",
        "        raise ValueError(\"Expected figure metadata to be a dictionary\")\n",
        "\n",
        "\n",
        "    return list(data.values())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq0Gqn7Wu03b",
        "outputId": "c3672803-e061-4e94-cfce-41f629e6b37b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"defined_total\": 90,\n",
            "  \"used_total\": 90,\n",
            "  \"unused_figures\": [],\n",
            "  \"invalid_figures\": []\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "vm_chunks = load_chunks_from_json(\"/data/VM_manual/final_VM_chunks.json\")\n",
        "vm_figures = load_figure_metadata_from_dict(\"/data/VM_manual/figure_metadata.json\")\n",
        "\n",
        "\n",
        "report = compare_figures(vm_figures, vm_chunks)\n",
        "print(json.dumps(report, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 344,
      "metadata": {
        "id": "401Qw6G_7ASu"
      },
      "outputs": [],
      "source": [
        "def find_chunk_references(fig_number: str, chunks: List[SemanticChunk]):\n",
        "    results = []\n",
        "    pattern = re.compile(rf\"(Figura\\s+{fig_number}\\s*-\\s*[^\\n]+)\", re.IGNORECASE)\n",
        "\n",
        "    for chunk in chunks:\n",
        "        matches = pattern.findall(chunk.content)\n",
        "        if matches:\n",
        "            results.append({\n",
        "                \"section\": chunk.section_number,\n",
        "                \"title\": chunk.section_title,\n",
        "                \"matched_lines\": matches,\n",
        "                \"attached_figures\": [fig.figure_id if hasattr(fig, \"figure_id\") else fig[\"figure_id\"] for fig in chunk.figures]\n",
        "            })\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9p42ems7DLw",
        "outputId": "1a8ad31d-6b03-4bd3-f2fe-9b938073bf26"
      },
      "outputs": [],
      "source": [
        "for fig_num in [\"21\", \"30\", \"44\"]:\n",
        "    print(f\"\\n Figura {fig_num} references:\")\n",
        "    refs = find_chunk_references(fig_num, vm_chunks)\n",
        "    for r in refs:\n",
        "        print(f\"\\n {r['section']} â€“ {r['title']}\")\n",
        "        for line in r[\"matched_lines\"]:\n",
        "            print(f\" {line}\")\n",
        "        print(f\" Attached figures: {r['attached_figures']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nawc3uzr0p6L"
      },
      "source": [
        "ahhhh we have 3 figure 69 but we match on figure num and then just attribute the first figure match!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbCBisDxw4W3",
        "outputId": "e098a679-4688-4a64-8304-c9e8c4d621a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"defined_total\": 49,\n",
            "  \"used_total\": 49,\n",
            "  \"unused_figures\": [],\n",
            "  \"invalid_figures\": []\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "wifi_chunks = load_chunks_from_json(\"/data/wifi_manual/final_wifi_chunks.json\")\n",
        "wifi_figures = load_figure_metadata_from_dict(\"/data/wifi_manual/figure_metadata.json\")\n",
        "\n",
        "# Run figure comparison\n",
        "report = compare_figures(wifi_figures, wifi_chunks)\n",
        "print(json.dumps(report, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 323,
      "metadata": {
        "id": "XSSOw9Ug0z-R"
      },
      "outputs": [],
      "source": [
        "test_chunk1 = {\n",
        "    \"section_number\": \"5.4.1.1.6\",\n",
        "    \"section_title\": \"NAT\",\n",
        "    \"content\": \"##### 5.4.1.1.6 NAT\\n\\n\\n\\n\\n\\n\\nNessuna regola NAT definita. Ãˆ possibile iniziare facendo clic su \\\"Aggiungi regola NAT\\\"\\n\\nFigura 68 - VMWare NSX â€“ Rete â€“ NAT\\n\\nSono presenti nella griglia i seguenti campi:\\n\\n- Nome.\\n\\n- Azione.\\n\\n- Associa.\\n\\n- IP di origine.\\n\\n- IP di destinazione/Porta.\\n\\n- IP convertito/Porta.\\n\\n- Applica a.\\n\\n- Abilitata.\\n\\n- Stato.\\n\\n\\n\\n*Pagina 48 di 66 (22/10/2024)*\\n\\n\\nFigura 69 - VMWare NSX â€“ Rete â€“ NAT â€“ Configurazione NAT\\n\\nQuesta funzionalitÃ  Ã¨ utile per configurare NAT nel contesto di un gateway di Livello 1 predefinito.\\nAl momento non sono presenti NAT configurati.\",\n",
        "    \"parent_topics\": [\n",
        "      \"MODALITÃ€ ESECUTIVE\",\n",
        "      \"CLOUD FIREWALL\",\n",
        "      \"PROFILI\"\n",
        "    ],\n",
        "    \"document\": \"ISTRUZIONE OPERATIVA CREAZIONE VM CLOUD INSIEL REV 00\",\n",
        "    \"chunk_summary\": \"This section of the technical manual, titled 'MODALITÃ€ ESECUTIVE > CLOUD FIREWALL > PROFILI > NAT', focuses on the configuration and management of Network Address Translation (NAT) rules within a VMWare NSX environment. It begins by indicating that no NAT rules are currently defined and provides a prompt to add new NAT rules. Key fields listed for rule configuration include Name, Action, Bind, Source IP, Destination IP/Port, Translated IP/Port, Apply to, Enabled, and Status. Two figures are included: one showing the NAT network setup and another detailing the NAT configuration steps, emphasizing the utility of NAT in the context of a predefined Level 1 gateway.\",\n",
        "    \"figures\": []\n",
        "}\n",
        "\n",
        "test_chunk2 = {\n",
        "    \"section_number\": \"5.4.1.1.7.1.\",\n",
        "    \"section_title\": \"SERVIZI DNS\",\n",
        "    \"content\": \"\"\"#### 5.4.1.1.7.1.SERVIZI DNS\n",
        "\n",
        "Figura 69 - VMWare NSX â€“ DNS â€“ Servizi DNS\n",
        "\n",
        "Sono presenti nella griglia i seguenti campi:\n",
        "\n",
        "- Nome\n",
        "\n",
        "- Gateway\n",
        "- IP servizio DNS\n",
        "- Zona DNS predefinita\n",
        "- Zona FQDN\n",
        "- Stato\n",
        "\n",
        "Figura 69 - VMWare NSX â€“ DNS â€“ Servizi DNS â€“ Configurazione DNS\n",
        "\n",
        "Non vi sono servizi DNS configurati a livello di NSX.\n",
        "\n",
        "NOTA BENE:\n",
        "- I servizi DNS vengono forniti nel contesto dell'Active Directory.\n",
        "- Tipicamente i servizi DNS vengono configurati manualmente sia per le VM collegate al dominio che per le VM non collegate al dominio.\"\"\",\n",
        "    \"figures\": []\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {
        "id": "pDLRDUZy04kn"
      },
      "outputs": [],
      "source": [
        "def match_caption_to_content(fig, content: str) -> bool:\n",
        "    lines = re.findall(r\"(Figura\\s+\\d+\\s*-\\s*[^\\n]+)\", content, flags=re.IGNORECASE)\n",
        "    fig_caption = fig[\"caption\"].lower()\n",
        "    return any(fig_caption in line.lower() for line in lines)\n",
        "\n",
        "def simulate_attach_figures(chunk, figure_metadata_list):\n",
        "    fig_index = {}\n",
        "    for fig in figure_metadata_list:\n",
        "        match = re.search(r\"Figura\\s+(\\d+)\", fig[\"figure_id\"])\n",
        "        if match:\n",
        "            fig_index.setdefault(match.group(1), []).append(fig)\n",
        "\n",
        "    matched_figures = []\n",
        "    seen_ids = set()\n",
        "\n",
        "    figure_nums = re.findall(r\"Figura\\s+(\\d+)\", chunk[\"content\"], flags=re.IGNORECASE)\n",
        "\n",
        "    for num in figure_nums:\n",
        "        for fig in fig_index.get(num, []):\n",
        "            if fig[\"figure_id\"] not in seen_ids and match_caption_to_content(fig, chunk[\"content\"]):\n",
        "                matched_figures.append(fig)\n",
        "                seen_ids.add(fig[\"figure_id\"])\n",
        "\n",
        "    return matched_figures\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buCEfD1Z6-1z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 324,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9dnkgCg06pl",
        "outputId": "0ecf8cf0-ca2a-4a5c-feb8-1475ff1fab74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Matched: Figura 68_vmware_nsx__rete__nat\n",
            "âœ… Matched: Figura 69_vmware_nsx__rete__nat__configurazione_nat\n"
          ]
        }
      ],
      "source": [
        "matched = simulate_attach_figures(test_chunk1, vm_figures)\n",
        "\n",
        "for fig in matched:\n",
        "    print(f\"âœ… Matched: {fig['figure_id']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 325,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6zv2cH-0_z5",
        "outputId": "063fbf80-85a9-4f7e-c1c1-932f66a9f971"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Matched: Figura 69_vmware_nsx__dns__servizi_dns\n",
            "âœ… Matched: Figura 69_vmware_nsx__dns__servizi_dns__configurazione_dns\n"
          ]
        }
      ],
      "source": [
        "matched = simulate_attach_figures(test_chunk2, vm_figures)\n",
        "\n",
        "for fig in matched:\n",
        "    print(f\"âœ… Matched: {fig['figure_id']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFwS8OxnK3-w"
      },
      "source": [
        "Clean JSON even more to seperate some metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-WWhGnoeRhzi"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "def clean_whitespace(text: str) -> str:\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "    text = re.sub(r'[ \\t]+$', '', text, flags=re.MULTILINE)\n",
        "    return text.strip()\n",
        "\n",
        "def determine_section_type(section_title: str) -> str:\n",
        "    title_lower = section_title.lower()\n",
        "\n",
        "    if any(word in title_lower for word in [\"configurazione\", \"config\", \"setup\"]):\n",
        "        return \"configuration\"\n",
        "    elif any(word in title_lower for word in [\"verifica\", \"test\", \"check\"]):\n",
        "        return \"verification\"\n",
        "    elif any(word in title_lower for word in [\"dettaglio\", \"detail\"]):\n",
        "        return \"detail\"\n",
        "    elif any(word in title_lower for word in [\"introduzione\", \"scopo\", \"ambito\"]):\n",
        "        return \"introduction\"\n",
        "    elif any(word in title_lower for word in [\"riferimenti\", \"allegati\", \"indice\"]):\n",
        "        return \"reference\"\n",
        "    else:\n",
        "        return \"general\"\n",
        "\n",
        "def transform_chunk(chunk):\n",
        "    section_id = chunk.get(\"section_number\", \"\")\n",
        "    section_title = chunk.get(\"section_title\", \"\")\n",
        "    page_numbers = chunk.get(\"page_numbers\", [])\n",
        "\n",
        "    if not page_numbers:\n",
        "        page_range = \"\"\n",
        "    elif len(page_numbers) == 1:\n",
        "        page_range = str(page_numbers[0])\n",
        "    else:\n",
        "        page_range = f\"{min(page_numbers)}-{max(page_numbers)}\"\n",
        "\n",
        "    parent_path = \" > \".join(chunk.get(\"parent_topics\", []))\n",
        "\n",
        "    content = clean_whitespace(chunk.get(\"content\", \"\"))\n",
        "    content_summary = chunk.get(\"chunk_summary\", \"\").strip()\n",
        "\n",
        "    figures = []\n",
        "    for fig in chunk.get(\"figures\", []):\n",
        "        figures.append({\n",
        "            \"figure_id\": fig.get(\"figure_id\", \"\").replace(\" \", \"_\"),\n",
        "            \"filename\": fig.get(\"filename\", \"\"),\n",
        "            \"caption\": fig.get(\"caption\", \"\").capitalize(),\n",
        "            \"page\": fig.get(\"page\", \"\")\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        \"section_id\": section_id,\n",
        "        \"section_title\": section_title,\n",
        "        \"page_range\": page_range,\n",
        "        \"parent_path\": parent_path,\n",
        "        \"document\": chunk.get(\"document\", \"\"),\n",
        "        \"content\": content,\n",
        "        \"content_summary\": content_summary,\n",
        "        \"figures\": figures,\n",
        "        \"metadata\": {\n",
        "            \"original_section_number\": section_id,\n",
        "            \"original_page_numbers\": page_numbers,\n",
        "            \"language\": \"Italian\",\n",
        "            \"section_type\": determine_section_type(section_title)\n",
        "        }\n",
        "    }\n",
        "\n",
        "def process_manual(input_file, output_file):\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    transformed = [transform_chunk(chunk) for chunk in data]\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(transformed, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"âœ“ {len(transformed)} chunks written to {output_file}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1KSFAaNUsmL",
        "outputId": "416ff1b1-ab32-4f75-e080-206c53de29f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ 46 chunks written to /content/drive/MyDrive/rag_comparison/data/wifi_manual/cleaned_wifi_chunks.json\n"
          ]
        }
      ],
      "source": [
        "wifi_input = \"/data/wifi_manual/final_wifi_chunks.json\"\n",
        "wifi_output = \"/data/wifi_manual/cleaned_wifi_chunks.json\"\n",
        "process_manual(wifi_input, wifi_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ-b23eIWxta",
        "outputId": "3897e3c0-3bb4-442b-837c-353c0b3cdc38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ 92 chunks written to /content/drive/MyDrive/rag_comparison/data/VM_manual/cleaned_VM_chunks.json\n"
          ]
        }
      ],
      "source": [
        "vm_input = \"/data/VM_manual/final_VM_chunks.json\"\n",
        "vm_output = \"/data/VM_manual/cleaned_VM_chunks.json\"\n",
        "process_manual(vm_input, vm_output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
