"""
Test suite for evaluating RAG system performance.

This module implements various metrics and evaluation methods for assessing
the quality and performance of the RAG system.
"""

import json
import time
import numpy as np
from typing import Dict, List, Tuple, Any
from pathlib import Path
from dataclasses import dataclass
from datetime import datetime
import psutil
import re
from sentence_transformers import SentenceTransformer
from openai import OpenAI

@dataclass
class TestResult:
    """Container for test results with metadata."""
    question_id: str
    category: str
    complexity: str
    query_type: str
    retrieval_time: float
    generation_time: float
    text_accuracy: float
    image_accuracy: float
    answer_quality: Dict[str, float]
    resource_usage: Dict[str, float]
    token_usage: Dict[str, int]  # Added token usage tracking
    timestamp: str

class RAGTestSuite:
    """Test suite for evaluating RAG system performance."""
    
    def __init__(self, questions_path: str):
        """Initialize the test suite with evaluation questions."""
        self.questions = self._load_questions(questions_path)
        self.results: List[TestResult] = []
    
    def _load_questions(self, path: str) -> List[Dict[str, Any]]:
        """Load evaluation questions from JSON file."""
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data['questions']
    
    def evaluate_retrieval_accuracy(self, 
                                  retrieved_sections: List[str],
                                  retrieved_figures: List[str],
                                  ground_truth_sections: List[str],
                                  ground_truth_figures: List[str]) -> Tuple[float, float]:
        """Evaluate retrieval accuracy using semantic similarity.
        
        Args:
            retrieved_sections: List of retrieved section texts
            retrieved_figures: List of retrieved figure paths
            ground_truth_sections: List of ground truth section texts
            ground_truth_figures: List of ground truth figure descriptions
            
        Returns:
            Tuple of (text_accuracy, image_accuracy)
            - text_accuracy: Semantic similarity between retrieved and ground truth sections
            - image_accuracy: Semantic similarity between retrieved and ground truth figure descriptions
        """
        model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # Text accuracy using semantic similarity
        if not retrieved_sections or not ground_truth_sections:
            text_accuracy = 0.0
        else:
            # Encode all sections
            retrieved_embeddings = model.encode(retrieved_sections)
            ground_truth_embeddings = model.encode(ground_truth_sections)
            
            # Calculate similarity matrix
            similarity_matrix = np.dot(retrieved_embeddings, ground_truth_embeddings.T)
            
            # Get best matches
            max_similarities = np.max(similarity_matrix, axis=1)
            text_accuracy = float(np.mean(max_similarities))
        
        # Image accuracy using semantic similarity
        if not retrieved_figures or not ground_truth_figures:
            image_accuracy = 0.0
        else:
            # Encode figure descriptions
            retrieved_embeddings = model.encode(retrieved_figures)
            ground_truth_embeddings = model.encode(ground_truth_figures)
            
            # Calculate similarity matrix
            similarity_matrix = np.dot(retrieved_embeddings, ground_truth_embeddings.T)
            
            # Get best matches
            max_similarities = np.max(similarity_matrix, axis=1)
            image_accuracy = float(np.mean(max_similarities))
        
        return text_accuracy, image_accuracy
    
    def evaluate_answer_quality(self, 
                              generated_answer: str,
                              ground_truth: str) -> Dict[str, float]:
        """Evaluate answer quality using LLM-based assessment.
        
        Args:
            generated_answer: The answer generated by the RAG system
            ground_truth: The ground truth answer
            
        Returns:
            Dictionary with quality metrics:
            - semantic_accuracy: How well the answer captures key points (0-1)
            - technical_correctness: How accurate the technical details are (0-1)
            - completeness: How complete the answer is (0-1)
            - clarity: How clear and well-structured the answer is (0-1)
            - relevance: How focused the answer is on the question (0-1)
        """
        client = OpenAI()
        
        # Create evaluation prompt
        evaluation_prompt = f"""
        Evaluate the following answer to a technical question. The answer should be evaluated on multiple dimensions.
        
        Question: {ground_truth}
        Generated Answer: {generated_answer}
        
        Please evaluate the answer on these dimensions (0-1 scale):
        1. Semantic Accuracy: Does the answer capture the key points and main ideas correctly?
        2. Technical Correctness: Are the technical details, numbers, and specifications accurate?
        3. Completeness: Does the answer cover all necessary aspects of the question?
        4. Clarity: Is the answer well-structured, clear, and easy to understand?
        5. Relevance: Does the answer stay focused on the question without digressing?
        
        Return your evaluation as a JSON object with these exact keys and values between 0 and 1:
        {{
            "semantic_accuracy": <value>,
            "technical_correctness": <value>,
            "completeness": <value>,
            "clarity": <value>,
            "relevance": <value>
        }}
        """
        
        # Get LLM evaluation
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are an expert technical evaluator. Always respond with valid JSON."},
                {"role": "user", "content": evaluation_prompt}
            ]
        )
        
        # Parse evaluation
        try:
            evaluation = json.loads(response.choices[0].message.content)
        except json.JSONDecodeError:
            # Fallback to default values if JSON parsing fails
            evaluation = {
                'semantic_accuracy': 0.5,
                'technical_correctness': 0.5,
                'completeness': 0.5,
                'clarity': 0.5,
                'relevance': 0.5
            }
        
        return {
            'semantic_accuracy': round(evaluation['semantic_accuracy'], 2),
            'technical_correctness': round(evaluation['technical_correctness'], 2),
            'completeness': round(evaluation['completeness'], 2),
            'clarity': round(evaluation['clarity'], 2),
            'relevance': round(evaluation['relevance'], 2)
        }
    
    def measure_resource_usage(self) -> Dict[str, float]:
        """Measure current resource usage.
        
        Returns:
            Dictionary with resource usage metrics:
            - memory_usage: Memory usage in MB
            - cpu_usage: CPU usage percentage
        """
        process = psutil.Process()
        
        # Get memory usage in MB
        memory_info = process.memory_info()
        memory_usage = memory_info.rss / (1024 * 1024)  # Convert to MB
        
        # Get CPU usage percentage
        cpu_usage = process.cpu_percent(interval=0.1)  # Measure over 0.1 seconds
        
        return {
            'memory_usage': round(memory_usage, 2),
            'cpu_usage': round(cpu_usage, 2)
        }
    
    def run_test(self, question: Dict[str, Any], approach: Any) -> TestResult:
        """
        Run a single test with the given approach.
        
        Args:
            question: The question to test
            approach: The RAG approach to test
            
        Returns:
            TestResult containing the results
        """
        try:
            # Get language from question, default to english
            language = question.get("language", "english")
            
            # Run the test
            start_time = time.time()
            answer, metrics = approach.answer_question(question["question"], language=language)
            end_time = time.time()
            
            # Calculate metrics
            response_time = end_time - start_time
            text_accuracy = self._calculate_text_accuracy(answer, question.get("answer", ""))
            image_accuracy = self._calculate_image_accuracy(metrics.get("retrieved_figures", []), question.get("ground_truth_figures", []))
            
            # Create result
            result = TestResult(
                question_id=question["id"],
                question=question["question"],
                generated_answer=answer,
                ground_truth=question.get("answer", ""),
                response_time=response_time,
                text_accuracy=text_accuracy,
                image_accuracy=image_accuracy,
                retrieved_sections=metrics.get("retrieved_sections", []),
                retrieved_figures=metrics.get("retrieved_figures", []),
                ground_truth_sections=question.get("ground_truth_sections", []),
                ground_truth_figures=question.get("ground_truth_figures", []),
                token_usage=metrics.get("total_tokens", 0),
                error=False,
                error_type=None,
                error_details=None,
                language=language
            )
            
            return result
            
        except Exception as e:
            # Handle errors
            error_type = "retrieval_failure" if isinstance(e, RetrievalError) else "generation_failure"
            return TestResult(
                question_id=question["id"],
                question=question["question"],
                generated_answer="",
                ground_truth=question.get("answer", ""),
                response_time=0,
                text_accuracy=0,
                image_accuracy=0,
                retrieved_sections=[],
                retrieved_figures=[],
                ground_truth_sections=question.get("ground_truth_sections", []),
                ground_truth_figures=question.get("ground_truth_figures", []),
                token_usage=0,
                error=True,
                error_type=error_type,
                error_details=str(e),
                language=question.get("language", "english")
            )
    
    def run_all_tests(self, rag_system: Any) -> List[TestResult]:
        """Run all tests in the suite."""
        return [self.run_test(q, rag_system) for q in self.questions]
    
    def generate_report(self) -> Dict[str, Any]:
        """Generate a comprehensive evaluation report."""
        if not self.results:
            return {"error": "No test results available"}
        
        # Calculate aggregate metrics
        metrics = {
            'average_retrieval_time': np.mean([r.retrieval_time for r in self.results]),
            'average_generation_time': np.mean([r.generation_time for r in self.results]),
            'average_text_accuracy': np.mean([r.text_accuracy for r in self.results]),
            'average_image_accuracy': np.mean([r.image_accuracy for r in self.results]),
            'average_answer_quality': {
                'semantic_accuracy': np.mean([r.answer_quality['semantic_accuracy'] for r in self.results]),
                'technical_correctness': np.mean([r.answer_quality['technical_correctness'] for r in self.results]),
                'completeness': np.mean([r.answer_quality['completeness'] for r in self.results]),
                'clarity': np.mean([r.answer_quality['clarity'] for r in self.results]),
                'relevance': np.mean([r.answer_quality['relevance'] for r in self.results])
            },
            'average_token_usage': {
                'retrieval': np.mean([r.token_usage['retrieval'] for r in self.results]),
                'generation': np.mean([r.token_usage['generation'] for r in self.results]),
                'total': np.mean([r.token_usage['total'] for r in self.results])
            }
        }
        
        # Group results by category and complexity
        category_results = {}
        complexity_results = {}
        
        for result in self.results:
            if result.category not in category_results:
                category_results[result.category] = []
            category_results[result.category].append(result)
            
            if result.complexity not in complexity_results:
                complexity_results[result.complexity] = []
            complexity_results[result.complexity].append(result)
        
        return {
            'overall_metrics': metrics,
            'category_breakdown': {
                cat: {
                    'count': len(results),
                    'average_text_accuracy': np.mean([r.text_accuracy for r in results]),
                    'average_image_accuracy': np.mean([r.image_accuracy for r in results]),
                    'average_answer_quality': {
                        'semantic_accuracy': np.mean([r.answer_quality['semantic_accuracy'] for r in results]),
                        'technical_correctness': np.mean([r.answer_quality['technical_correctness'] for r in results]),
                        'completeness': np.mean([r.answer_quality['completeness'] for r in results]),
                        'clarity': np.mean([r.answer_quality['clarity'] for r in results]),
                        'relevance': np.mean([r.answer_quality['relevance'] for r in results])
                    },
                    'average_token_usage': np.mean([r.token_usage['total'] for r in results])
                }
                for cat, results in category_results.items()
            },
            'complexity_breakdown': {
                comp: {
                    'count': len(results),
                    'average_text_accuracy': np.mean([r.text_accuracy for r in results]),
                    'average_image_accuracy': np.mean([r.image_accuracy for r in results]),
                    'average_answer_quality': {
                        'semantic_accuracy': np.mean([r.answer_quality['semantic_accuracy'] for r in results]),
                        'technical_correctness': np.mean([r.answer_quality['technical_correctness'] for r in results]),
                        'completeness': np.mean([r.answer_quality['completeness'] for r in results]),
                        'clarity': np.mean([r.answer_quality['clarity'] for r in results]),
                        'relevance': np.mean([r.answer_quality['relevance'] for r in results])
                    },
                    'average_token_usage': np.mean([r.token_usage['total'] for r in results])
                }
                for comp, results in complexity_results.items()
            },
            'detailed_results': [
                {
                    'question_id': r.question_id,
                    'category': r.category,
                    'complexity': r.complexity,
                    'query_type': r.query_type,
                    'retrieval_time': r.retrieval_time,
                    'generation_time': r.generation_time,
                    'text_accuracy': r.text_accuracy,
                    'image_accuracy': r.image_accuracy,
                    'answer_quality': r.answer_quality,
                    'token_usage': r.token_usage,
                    'timestamp': r.timestamp
                }
                for r in self.results
            ]
        }
    
    def save_report(self, output_path: str):
        """Save the evaluation report to a file."""
        report = self.generate_report()
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False) 